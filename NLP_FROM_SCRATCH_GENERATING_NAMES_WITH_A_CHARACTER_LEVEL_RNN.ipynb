{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP FROM SCRATCH: GENERATING NAMES WITH A CHARACTER-LEVEL RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP7iUdR75FxrOnvbFVoHdo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/Pytorch-study/blob/master/NLP_FROM_SCRATCH_GENERATING_NAMES_WITH_A_CHARACTER_LEVEL_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_3xEYdFL2eN",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "이번 튜토리얼은 밑바닥부터 NLP 시작하기 (NLP from scratch)의 두번째 시간입니다. 첫번째 튜토리얼에선 RNN을 이용하여 이름을 갖고 언어를 구분하는 예제를 진행했었습니다. 이번시간에는 이름을 생성해보도록 하겠습니다.\n",
        "\n",
        "> 추가 설명 및 부연 설명은 이 글과 같이 citation으로 달겠습니다.   \n",
        "원본은 https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html 를 참고해주세요.\n",
        "\n",
        "```\n",
        "> python sample.py Russian RUS\n",
        "Rovakov\n",
        "Uantov\n",
        "Shavakov\n",
        "\n",
        "> python sample.py German GER\n",
        "Gerren\n",
        "Ereng\n",
        "Rosher\n",
        "\n",
        "> python sample.py Spanish SPA\n",
        "Salla\n",
        "Parer\n",
        "Allan\n",
        "\n",
        "> python sample.py Chinese CHI\n",
        "Chan\n",
        "Hang\n",
        "Iun\n",
        "```\n",
        "\n",
        "저희는 여전히 몇 개의 linear layer로 구성된 직접 만든 RNN을 사용할 것입니다. 이전 튜토리얼과 가장 큰 차이점은 이름에 있는 모든 글자를 input으로 넣고 category를 예측하는 것이 아닌, category를 input으로 매 시점에서의 글자를 예측하는 것입니다. 언어를 구성하기 위해 글자들을 반복하여 예측하는 것을 (이는 단어나 어떤 상위 개념에도 똑같이 적용됩니다) **language modeling**이라 부릅니다.\n",
        "\n",
        "**Recomended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
        "\n",
        "- https://pytorch.org/ For installation instructions\n",
        "- Deep Learning with PyTorch: A 60 Minute Blitz to get started with PyTorch in general\n",
        "- Learning PyTorch with Examples for a wide and deep overview\n",
        "- PyTorch for Former Torch Users if you are former Lua Torch user\n",
        "\n",
        "It would also be useful to know about RNNs and how they work:\n",
        "\n",
        "- The Unreasonable Effectiveness of Recurrent Neural Networks shows a bunch of real life examples\n",
        "- Understanding LSTM Networks is about LSTMs specifically but also informative about RNNs in general\n",
        "- I also suggest the previous tutorial, NLP From Scratch: Classifying Names with a Character-Level RNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8WzCd2Wrf1x",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparing\n",
        "데이터는 [이곳](https://download.pytorch.org/tutorial/data.zip)에서 다운받을 수 있습니다.\n",
        "> 이전 튜토리얼과 마찬가지로, linux 명령어를 통해 colab의 메모리 혹은 디렉토리에 바로 다운받을 수 있게끔 하겠습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erTicU2VQlak",
        "colab_type": "code",
        "outputId": "178b9dfb-c507-493e-9a76-dcf18e635505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-26 08:34:47--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.249.87.81, 13.249.87.32, 13.249.87.127, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.249.87.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  17.5MB/s    in 0.2s    \n",
            "\n",
            "2020-01-26 08:34:47 (17.5 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c8OSiAN967m",
        "colab_type": "text"
      },
      "source": [
        "이전 튜토리얼에서 작성했던 전처리 함수를 그대로 가져다 쓰겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It6F_dHVr33Z",
        "colab_type": "code",
        "outputId": "8bd59cf5-a957-409e-f60e-4167d4c4128b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1 # EOS (End of Sentence) mark를 달겠습니다.\n",
        "\n",
        "def find_files(path): return glob.glob(path)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# 파일을 읽고 lines로 split하겠습니다.\n",
        "def read_lines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicode_to_ascii(line) for line in lines]\n",
        "\n",
        "# category_lines 딕셔너리를 생성하여 category 당 lines의 list를 저장합니다.\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "for filename in find_files('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = read_lines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "# category가 0일 경우, exception을 raise합니다.\n",
        "if n_categories == 0:\n",
        "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
        "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
        "        'the current directory.')\n",
        "\n",
        "print('# categories:', n_categories, all_categories)\n",
        "print(unicode_to_ascii(\"O'Néàl\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# categories: 18 ['Irish', 'Dutch', 'Japanese', 'Greek', 'Korean', 'Polish', 'Vietnamese', 'Arabic', 'Scottish', 'Portuguese', 'Russian', 'German', 'Chinese', 'Spanish', 'French', 'Italian', 'Czech', 'English']\n",
            "O'Neal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elgqS7nZEJW-",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Network\n",
        "이번 네트워크는 [이전 튜토리얼의 RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#Creating-the-Network)을 확장한 것으로, cateogry tensor를 위한 추가 argument가 존재합니다. 이 또한 one-hot vetor로, 다른 tensor와 concatenate됩니다.\n",
        "\n",
        "여기서 output은 다음 글자의 확률로 해석됩니다. 샘플링할 때, 가장 그럴듯한 output 글자는 다음 input 글자가됩니다.\n",
        "\n",
        "이번 튜토리얼에서는 두번째 linear layer인 `o2o`(output과 hidden을 합친 후에)가 추가되어 더 많은 기능을 하게됩니다. 또한, overfiting을 방지하는 dropout layer가 있습니다. 여기서는 네트워크의 끝부분에서 이용되어 의도적으로 혼란을 추가하고 sampling variety를 증가시킵니다.\n",
        "![](https://i.imgur.com/jzVrf7f.png)\n",
        "\n",
        "> 이번 예제는 Seq2seq와 비슷합니다.\n",
        "\n",
        "> 밑에는 이전 튜토리얼의 네트워크입니다. 한번 보고 다른점을 한번 확인해보세요.\n",
        "\n",
        "![](https://i.imgur.com/Z2xbySO.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReF2wRlZDvTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # dimension이 헷갈린다면 그림을 확인해보세요.\n",
        "        self.i2h = nn.Linear(input_size + self.hidden_size + n_categories, self.hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + self.hidden_size + n_categories, output_size)\n",
        "        self.o2o = nn.Linear(self.hidden_size + output_size, output_size)\n",
        "        # Dropout layer를 추가합니다.\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, category, input, hidden):\n",
        "        input_combined = torch.cat((category, input, hidden), dim=1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "\n",
        "        output_combined = torch.cat((hidden, output), dim=1)\n",
        "        output = self.o2o(output_combined)\n",
        "        output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9SvshJNLf55",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "## Preparing for training\n",
        "가장 우선적으로, (category, line)으로 이루어진 쌍을 랜덤으로 얻는 helper functions를 만들어보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aFa0TDwLe4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "# list로부터 random하게 요소 뽑아내기\n",
        "def random_choice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# 카테고리로부터 random line과 random category 뽑아내기\n",
        "def random_training_pair():\n",
        "    # 카테고리 먼저 랜덤으로 뽑고,\n",
        "    category = random_choice(all_categories)\n",
        "    # 뽑은 카테고리에 저장된 이름(line)을 가져옵니다.\n",
        "    line = random_choice(category_lines[category])\n",
        "    return category, line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLIrzanOgjk",
        "colab_type": "text"
      },
      "source": [
        "매 timestep(training할 단어의 각 글자)에서, 네트워크의 input은 `(category, current_letter, hidden state)`가 될 것이고, output은 `(next letter, next hidden state)`가 될 것입니다. 따라서 각 training set에 대해, category와 input letter, output/target letters가 필요합니다.\n",
        "\n",
        "매 timestep에 대해, 현재 글자로부터 다음 글자를 예측해야되기 때문에 글자 쌍은 lines로부터 얻어진 연속적인 글자가 됩니다 - e.g. `ABCD<EOS>`에 대해, (\"A\", \"B\"), (\"B\", \"C\"), (\"C\", \"D\"), (\"D\", \"EOS\")쌍을 만들면 됩니다.\n",
        "\n",
        "![](https://i.imgur.com/JH58tXY.png)\n",
        "\n",
        "Category 텐서는 one-hot tensor로, `<1 x n_categories>`의 사이즈를 갖고 있습니다. Training 과정에서 이를 모든 timestep에서 네트워크에 집어넣습니다(feed) - 이는 초기 hidden state나 어떤 다른 전략의 일부분으로 포함될 수 있는 것으로, 선택하기 나름입니다(design choice).\n",
        "\n",
        "> Design choice는 설계 스타일을 의미하는 것 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTPMS5-1Ofoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 카테고리를 one-hot vector로 바꾸기\n",
        "def category_tensor(category):\n",
        "    li = all_categories.index(category)\n",
        "    tensor = torch.zeros(1, n_categories)\n",
        "    tensor[0][li] = 1\n",
        "    return tensor\n",
        "\n",
        "# input의 첫번째부터 마지막 글자(EOS 제외)의 one-hot matrix\n",
        "def input_tensor(line):\n",
        "    '''\n",
        "    # 원본은 다음과 같이 되어 있으나, dimension을 편리하게 표현해보겠습니다.\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    '''\n",
        "    # <line_length x batch_size x n_letters>\n",
        "    line_length = len(line)\n",
        "    batch_size = 1\n",
        "    tensor = torch.zeros(line_length, batch_size, n_letters)\n",
        "    for li in range(line_length):\n",
        "        letter = line[li]\n",
        "        # 마지막 차원(n_letters)의 index만 1로 바꾸어주면 one-hot으로 표현 가능합니다.\n",
        "        tensor[li][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Target의 두번째 글자부터 마지막(EOS)까지를 LongTensor로 표현합니다.\n",
        "def target_tensor(line):\n",
        "    # 첫번째 글자는 t=1일 때 input으로 주기 때문에 제외합니다.\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))] # list comprehension\n",
        "    letter_indexes.append(n_letters - 1) # EOS\n",
        "    return torch.LongTensor(letter_indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEkmRmkeYioe",
        "colab_type": "text"
      },
      "source": [
        "Training 과정에서 편의를 위해, 랜덤으로 (category, line) 쌍을 가져오는 `random_training_example` 함수를 만들고, 이를 필요한 (category, input, target) 텐서로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wEwxbmkXbVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (random_category, line) 쌍으로부터 category, input, target tensor를 만듭니다.\n",
        "def random_training_example():\n",
        "    # random하게 category와 line을 가져옵니다.\n",
        "    category, line = random_training_pair()\n",
        "    # category를 <1 x n_categories> tensor로 변환합니다.\n",
        "    category = category_tensor(category)\n",
        "    # 이름(line)을 <line_length  x batch_size x n_letters> tensor로 변환(EOS 제외)\n",
        "    input_line_tensor = input_tensor(line)\n",
        "    # 이름(line)을 <line_length  x batch_size x n_letters> tensor로 변환(EOS 포함, 첫 글자 제거)\n",
        "    target_line_tensor = target_tensor(line)\n",
        "    return category, input_line_tensor, target_line_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tUqA-hZavwW",
        "colab_type": "text"
      },
      "source": [
        "## Traning the Network\n",
        "오직 마지막 output만이 사용되는 classification 문제와는 반대로, 모든 스텝에서 예측하여 모든 스텝에서의 loss를 계산합니다.\n",
        "\n",
        "Autograd의 마술덕분에 우리는 매 순간에서의 losses를 단순히 더하고 마지막에 backward함수만 더하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcjHuqUTaumZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "    hidden = rnn.init_hidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l\n",
        "    \n",
        "    loss.backward()\n",
        "    '''\n",
        "    # 원본은 다음과 같이 gradient를 update합니다.\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)\n",
        "    '''\n",
        "    optimizer.step()\n",
        "\n",
        "    return output, loss.item() / input_line_tensor.size(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MazKqknWhMDq",
        "colab_type": "text"
      },
      "source": [
        "Training 과정에서 소요되는 시간을 기록하기 위해 `time_since(timestamp)`함수를 만들겠습니다. 이 함수는 사람이 읽을 수 있는 string을 반환합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSeTmBaNmJDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def time_since(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    # f string을 사용하겠습니다\n",
        "    return f'{m}m {s}s'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUdFDLZ6mT1s",
        "colab_type": "text"
      },
      "source": [
        "Training은 평상시와 같이 진행합니다 - train을 몇 번 호출하고, 몇 분 기다린 후, 매 `print_every`번째 마다 현재 시간과 loss를 출력하고, `plot_every`번째마다 평균 loss를 `all_losses`에 저장하여 추후에 plotting합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMxPUFpImS1y",
        "colab_type": "code",
        "outputId": "af58ea78-19f5-4007-89eb-e89bfeb151bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "\n",
        "# 위에서 직접 update를 하는 것이 아닌, optimizer로 접근했기 때문에 여기서 optimizer를 만들어줍니다.\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*random_training_example())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        print(f'{time_since(start)} ({iter} {iter / n_iters * 100}%) {loss:.4f}')\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 13.217996597290039s (5000 5.0%) 2.4005\n",
            "0m 26.537834644317627s (10000 10.0%) 3.1587\n",
            "0m 39.96205163002014s (15000 15.0%) 2.7366\n",
            "0m 53.25227069854736s (20000 20.0%) 2.1939\n",
            "1m 6.545685768127441s (25000 25.0%) 2.4484\n",
            "1m 19.706288814544678s (30000 30.0%) 2.9784\n",
            "1m 33.117918491363525s (35000 35.0%) 2.9835\n",
            "1m 46.60876774787903s (40000 40.0%) 3.1673\n",
            "1m 59.79396867752075s (45000 45.0%) 2.0760\n",
            "2m 13.181854248046875s (50000 50.0%) 2.9127\n",
            "2m 26.867607831954956s (55000 55.00000000000001%) 1.1377\n",
            "2m 40.70514440536499s (60000 60.0%) 3.0959\n",
            "2m 54.16928696632385s (65000 65.0%) 2.0902\n",
            "3m 7.504065275192261s (70000 70.0%) 1.8151\n",
            "3m 20.8232159614563s (75000 75.0%) 2.5841\n",
            "3m 34.19641709327698s (80000 80.0%) 2.0511\n",
            "3m 47.752302169799805s (85000 85.0%) 1.6212\n",
            "4m 1.099242925643921s (90000 90.0%) 1.8623\n",
            "4m 14.33629035949707s (95000 95.0%) 2.2072\n",
            "4m 27.73836398124695s (100000 100.0%) 1.7043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTJZRKmsogkI",
        "colab_type": "text"
      },
      "source": [
        "# Plotting the Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxoLk4jMhK5K",
        "colab_type": "code",
        "outputId": "74ceb8d2-7ad5-41e9-b3ba-ce48276dbf75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1f28909eb8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGC9nid4YY7ycJXpjjPFy/x9w83jVgnmi5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XhYSZR7on7O",
        "colab_type": "text"
      },
      "source": [
        "# Sampling the Network\n",
        "샘플링하기 위해서는 네트워크에 글자를 주고 다음 글자가 뭔지 예측하고, 이를 다시 넣어 그 다음 글자를 예측하고, 이를 EOS 토큰을 얻을 때까지 반복한다.\n",
        "- input category, 이름의 첫 글자, 비어있는 hidden state의 tensor를 생성한다\n",
        "- starting letter를 통해 `output_name`이라는 string을 생성한다\n",
        "- 최고 output 길이에 도달할 때까지\n",
        " - 현재 글자를 network에 입력한다\n",
        " - 최고 확률을 갖는 다음 글자, output과 hidden state를 얻는다\n",
        " - 글자가 EOS라면 종료한다\n",
        " - 일반적인 글자라면, `output_name`에 글자를 추가하고 계속한다\n",
        "- 최종 이름을 반환한다\n",
        "\n",
        "**Note**  \n",
        "시작 글자를 주는 대신, 훈련과정에서 \"Start of String\" 토큰을 주어 네트워크가 맨 처음 글자를 고르는 방법도 있습니다.\n",
        "\n",
        "> 일반적으로는 <BOS> 토큰이라고 부릅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVZr7Z0xomtv",
        "colab_type": "code",
        "outputId": "688739af-b1cd-48fc-e5dc-00c1d5175faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "max_length = 20\n",
        "\n",
        "# Sample from a category and starting letter\n",
        "def sample(category, start_letter='A'):\n",
        "    with torch.no_grad():\n",
        "        # input category, 이름의 첫 글자, 비어있는 hidden state의 tensor를 생성합니다.\n",
        "\n",
        "        # CamelCase를 Pythonic하게 바꾸는 과정에서 variable과 function 이름이 중복되었습니다.\n",
        "        # 따라서 category_tensor를 category라는 이름으로 저장하겠습니다.\n",
        "        # 원래라면 parameter와 동명의 이름을 갖기 때문에 좋은 코딩방법은 아닙니다.\n",
        "        category = category_tensor(category)\n",
        "        input = input_tensor(start_letter)\n",
        "        hidden = rnn.init_hidden()\n",
        "\n",
        "        # starting letter를 통해 output_name이라는 string을 생성한다\n",
        "        output_name = start_letter  # Immutable\n",
        "\n",
        "        for i in range(max_length):\n",
        "            # 현재 글자를 network에 입력한다\n",
        "            output, hidden = rnn(category, input[0], hidden)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi[0][0]\n",
        "\n",
        "            # 최고 output 길이에 도달할 때까지\n",
        "            if topi == n_letters - 1:\n",
        "                break\n",
        "            else:\n",
        "                # 일반적인 글자라면, output_name에 글자를 추가하고 계속한다\n",
        "                letter = all_letters[topi]\n",
        "                output_name += letter\n",
        "            input = input_tensor(letter)\n",
        "        \n",
        "        return output_name\n",
        "\n",
        "# Get multiple samples from one category and multiple starting letters\n",
        "def samples(category, start_letters=\"ABC\"):\n",
        "    for start_letter in start_letters:\n",
        "        print(sample(category, start_letter))\n",
        "\n",
        "samples('Russian', 'RUS')\n",
        "\n",
        "samples('German', 'GER')\n",
        "\n",
        "samples('Spanish', 'SPA')\n",
        "\n",
        "samples('Chinese', 'CHI')\n",
        "\n",
        "samples(\"Korean\", \"KOR\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rovaki\n",
            "Uanishin\n",
            "Sharivavivin\n",
            "Gerre\n",
            "Ererer\n",
            "Ronger\n",
            "Salla\n",
            "Perra\n",
            "Allara\n",
            "Can\n",
            "Han\n",
            "Iun\n",
            "Kon\n",
            "Oon\n",
            "Ron\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJyIiKZGyJ-3",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "- 다른 데이터 셋을 통해 category -> line을 예측해보십시오\n",
        " - 픽션 -> 캐릭터 이름\n",
        " - 품사 -> 단어\n",
        " - 나라 -> 도시  \n",
        "- \"Start of Sentence\" 토큰을 사용하여 시작 글자를 선택하지 않고 시도해봅니다 - 더 크거나 더 나은 네트워크를 이용해봅시다\n",
        " - nn.LSTM, nn.GRU 네트워크\n",
        " - 이러한 RNN 여러개를 묶어 높은 단계의 네트워크로 사용하기"
      ]
    }
  ]
}