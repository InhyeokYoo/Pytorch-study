{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM 실습코드.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/Pytorch-study/blob/master/LSTM_%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6lVCv43D2uY",
        "colab_type": "text"
      },
      "source": [
        "# 6.4 LSTM\n",
        "$$\n",
        "\\begin{array}{ll} \\\\\n",
        "            i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
        "            f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
        "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
        "            o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
        "            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
        "            h_t = o_t * \\tanh(c_t) \\\\\n",
        "        \\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOXqE-GLEOhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRH1wU68S_6V",
        "colab_type": "code",
        "outputId": "528a235e-4a91-419b-dcf2-c1cd0d6472d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# input_size:     입력의 특성 개수\n",
        "# hidden_size:    hidden state의 특성 개수\n",
        "# num_layers:     LSTM을 몇층으로 쌓을것인가 여부\n",
        "# bias:           편차의 사용 여부\n",
        "# batch_first:    사용하면 입력과 출력의 형태가 [batch, seq, feature]\n",
        "# dropout:        드롭아웃 사용여부\n",
        "# bidirectional:  참고 http://solarisailab.com/archives/1515\n",
        "\n",
        "rnn = nn.LSTM(input_size=3, hidden_size=5, num_layers=2)\n",
        "\n",
        "# 기본적으로 입력의 형태는 (seq_len, batch, input_size) 를 따른다.\n",
        "input_ = torch.randn(5, 3, 3)\n",
        "\n",
        "# hidden layer, cell state의 형태는 (num_layers * num_directions, batch, hidden_Size) 의 형태를 따름.\n",
        "h0 = torch.randn(2, 3, 5)\n",
        "c0 = torch.randn(2, 3, 5)\n",
        "\n",
        "# LSTM에 입력을 전달할 때는 input, (h_0, c_0) 처럼 상태를 튜플로 묶어서 전달한다.\n",
        "output, (hidden_state, cell_state) = rnn(input_, (h0, c0))\n",
        "\n",
        "print(output.size(), hidden_state.size(), cell_state.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 5]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DS5zRXZ1q9Q",
        "colab_type": "code",
        "outputId": "772785bd-72f3-48a2-b8ad-9427174425a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### 헷갈리니까 그냥 변수로 저장해서 불러오는게 나을듯.\n",
        "input_size = 3\n",
        "hidden_size = 5\n",
        "num_layers = 2\n",
        "seq_len = 5 # 이게 뭔지 잘 모르겠음.\n",
        "batch_size = 3\n",
        "\n",
        "rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "\n",
        "# 기본적으로 입력의 형태는 (seq_len, batch, input_size) 를 따른다.\n",
        "input_ = torch.randn(seq_len, batch_size, input_size)\n",
        "\n",
        "# hidden layer, cell state의 형태는 (num_layers * num_directions, batch, hidden_Size) 의 형태를 따름.\n",
        "h0 = torch.randn(num_layers * 1, batch_size, hidden_size)\n",
        "c0 = torch.randn(num_layers * 1, batch_size, hidden_size)\n",
        "\n",
        "# LSTM에 입력을 전달할 때는 input, (h_0, c_0) 처럼 상태를 튜플로 묶어서 전달한다.\n",
        "output, (hidden_state, cell_state) = rnn(input_, (h0, c0))\n",
        "\n",
        "# output:[seq_len, batch, num_directions * hidden_size] -> 왜?\n",
        "print(output.size(), hidden_state.size(), cell_state.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 5]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRRd671YFj6a",
        "colab_type": "text"
      },
      "source": [
        "## Hard Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU8kStUqFGSL",
        "colab_type": "code",
        "outputId": "9ed50577-0721-427e-99fd-31a7f7549bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "### 이번엔 batch_first를 한번 사용해보도록 하자.\n",
        "\n",
        "rnn = nn.LSTM(input_size=3, hidden_size=5, num_layers=2, batch_first=True)\n",
        "\n",
        "# batch_first=True이면 입력은 (batch, seq, input_size) 가 된다: batch랑 seq의 위치가 바뀜.\n",
        "input_ = torch.randn(3, 5, 3)\n",
        "\n",
        "# hidden state, cell state의 형태는 아까와 동일하게 (num_layers * num_directions, batch, hidden_size)\n",
        "h0 = torch.randn(2, 3, 5)\n",
        "c0 = torch.randn(2, 3, 5)\n",
        "\n",
        "# LSTM에 입력을 전달할 때는 위와 동일하게 input, (h_0, c_0) 로 전달하면 됨.\n",
        "\n",
        "# Q. 왜 여기서는 forward 호출 안하지?\n",
        "output, (hidden_state, cell_state) = rnn(input_, (h0, c0))\n",
        "\n",
        "print(input_.size(),h0.size(),c0.size())\n",
        "print(output.size(),hidden_state.size(),cell_state.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5, 3]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n",
            "torch.Size([3, 5, 5]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YsBtcrWP4Zn",
        "colab_type": "text"
      },
      "source": [
        "## Char_LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-w2jV34N5tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple Character LSTM\n",
        "# Char RNN에서 설명한 부분은 생략했습니다.\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA4PV-zfP8D_",
        "colab_type": "code",
        "outputId": "4353c36f-f31c-4e78-df83-8d86b42dc63e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Preprocessing string data\n",
        "# alphabet(0-25), space(26),..., start, end \n",
        "\n",
        "string = \"hello pytorch. how long can a rnn cell remember? show me your limit!\"\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n",
        "char_list = [i for i in chars]\n",
        "char_len = len(char_list)\n",
        "\n",
        "print(len(string), char_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83o-WFFQP91c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# String to onehot vector\n",
        "# a -> [1 0 0 ... 0 0]\n",
        "\n",
        "def string_to_onehot(string):\n",
        "    start = np.zeros(shape=char_len ,dtype=int)\n",
        "    end = np.zeros(shape=char_len ,dtype=int)\n",
        "    start[-2] = 1\n",
        "    end[-1] = 1\n",
        "    for i in string:\n",
        "        idx = char_list.index(i)\n",
        "        zero = np.zeros(shape=char_len ,dtype=int)\n",
        "        zero[idx]=1\n",
        "        start = np.vstack([start,zero])\n",
        "    output = np.vstack([start,end])\n",
        "    return output\n",
        "\n",
        "# Onehot vector to word\n",
        "# [1 0 0 ... 0 0] -> a \n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1wNdirYQBRp",
        "colab_type": "code",
        "outputId": "6bd7f78b-19ef-4c03-f750-e87c34c36e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Hyper-parameter 설정\n",
        "\n",
        "batch_size = 1 # 문자열을 하나씩 잘라서 사용하는 것으로하여 bacth_size=1로 고정하였음.\n",
        "\n",
        "# seq_len은 편의상 1로 설정 -> 이게 뭔지 잘 모르겠음.\n",
        "seq_len = 1\n",
        "\n",
        "# num_layers는 입력 형식에만 맞게 형태를 바꾸어 주면 됨. -> 뭔 소리임?\n",
        "num_layers = 3\n",
        "input_size = char_len\n",
        "hidden_size = 35\n",
        "lr = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "# string을 one_hot의 모음으로 바꿈: (start + 68 + end) x (35)\n",
        "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "\n",
        "print(one_hot.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 35])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibk8MMdRQpeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### LSTM with 1 hidden layer\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers) # Q. 이건 왜 있는거지?\n",
        "\n",
        "    def forward(self, input_, hidden, cell):\n",
        "        output, (hidden, cell) = self.lstm(input_, (hidden, cell))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def init_hidden_cell(self):\n",
        "        hidden = torch.zeros(num_layers,batch_size,hidden_size)\n",
        "        cell = torch.zeros(num_layers,batch_size,hidden_size)\n",
        "        return hidden,cell\n",
        "\n",
        "lstm = LSTM(input_size, hidden_size, num_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqxzkZUNRaXl",
        "colab_type": "code",
        "outputId": "f1524b81-b9f3-42e5-822d-8325f20fee17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "### Loss function & Optimizer\n",
        "\n",
        "loss_func = nn.MSELoss() \n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=lr)\n",
        "\n",
        "j=0\n",
        "input_data = one_hot[j:j+seq_len].view(seq_len, batch_size, input_size)\n",
        "print(input_data.size())\n",
        "\n",
        "hidden, cell = lstm.init_hidden_cell()\n",
        "print(hidden.size(),cell.size())\n",
        "\n",
        "output, hidden, cell = lstm(input_data, hidden,cell)\n",
        "print(output.size(),hidden.size(),cell.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 35])\n",
            "torch.Size([3, 1, 35]) torch.Size([3, 1, 35])\n",
            "torch.Size([1, 1, 35]) torch.Size([3, 1, 35]) torch.Size([3, 1, 35])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPQdrPPZRgqo",
        "colab_type": "code",
        "outputId": "1936db77-bae2-4b7e-bd7a-21b982f5075e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "unroll_len = one_hot.size()[0]//seq_len -1\n",
        "for i in range(num_epochs):\n",
        "    hidden,cell = lstm.init_hidden_cell()\n",
        "    \n",
        "    loss = 0\n",
        "    for j in range(unroll_len):\n",
        "        input_data = one_hot[j:j+seq_len].view(seq_len,batch_size,input_size) \n",
        "        label = one_hot[j+1:j+seq_len+1].view(seq_len,batch_size,input_size)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, hidden, cell = lstm(input_data,hidden,cell)\n",
        "        loss += loss_func(output.view(1,-1),label.view(1,-1))\n",
        "        \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i%10 ==0:\n",
        "        print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.1684, grad_fn=<AddBackward0>)\n",
            "tensor(1.8090, grad_fn=<AddBackward0>)\n",
            "tensor(1.7084, grad_fn=<AddBackward0>)\n",
            "tensor(1.5447, grad_fn=<AddBackward0>)\n",
            "tensor(1.2709, grad_fn=<AddBackward0>)\n",
            "tensor(0.8998, grad_fn=<AddBackward0>)\n",
            "tensor(0.5241, grad_fn=<AddBackward0>)\n",
            "tensor(0.2770, grad_fn=<AddBackward0>)\n",
            "tensor(0.1345, grad_fn=<AddBackward0>)\n",
            "tensor(0.0732, grad_fn=<AddBackward0>)\n",
            "tensor(0.0453, grad_fn=<AddBackward0>)\n",
            "tensor(0.0308, grad_fn=<AddBackward0>)\n",
            "tensor(0.0227, grad_fn=<AddBackward0>)\n",
            "tensor(0.0180, grad_fn=<AddBackward0>)\n",
            "tensor(0.0150, grad_fn=<AddBackward0>)\n",
            "tensor(0.0130, grad_fn=<AddBackward0>)\n",
            "tensor(0.0127, grad_fn=<AddBackward0>)\n",
            "tensor(0.0109, grad_fn=<AddBackward0>)\n",
            "tensor(0.0099, grad_fn=<AddBackward0>)\n",
            "tensor(0.0086, grad_fn=<AddBackward0>)\n",
            "tensor(0.0078, grad_fn=<AddBackward0>)\n",
            "tensor(0.0072, grad_fn=<AddBackward0>)\n",
            "tensor(0.0068, grad_fn=<AddBackward0>)\n",
            "tensor(0.0067, grad_fn=<AddBackward0>)\n",
            "tensor(0.0066, grad_fn=<AddBackward0>)\n",
            "tensor(0.0062, grad_fn=<AddBackward0>)\n",
            "tensor(0.0059, grad_fn=<AddBackward0>)\n",
            "tensor(0.0057, grad_fn=<AddBackward0>)\n",
            "tensor(0.0055, grad_fn=<AddBackward0>)\n",
            "tensor(0.0054, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0049, grad_fn=<AddBackward0>)\n",
            "tensor(0.0048, grad_fn=<AddBackward0>)\n",
            "tensor(0.0047, grad_fn=<AddBackward0>)\n",
            "tensor(0.0046, grad_fn=<AddBackward0>)\n",
            "tensor(0.0046, grad_fn=<AddBackward0>)\n",
            "tensor(0.0045, grad_fn=<AddBackward0>)\n",
            "tensor(0.0045, grad_fn=<AddBackward0>)\n",
            "tensor(0.0044, grad_fn=<AddBackward0>)\n",
            "tensor(0.0044, grad_fn=<AddBackward0>)\n",
            "tensor(0.0044, grad_fn=<AddBackward0>)\n",
            "tensor(0.0047, grad_fn=<AddBackward0>)\n",
            "tensor(0.0044, grad_fn=<AddBackward0>)\n",
            "tensor(0.0043, grad_fn=<AddBackward0>)\n",
            "tensor(0.0042, grad_fn=<AddBackward0>)\n",
            "tensor(0.0042, grad_fn=<AddBackward0>)\n",
            "tensor(0.0042, grad_fn=<AddBackward0>)\n",
            "tensor(0.0046, grad_fn=<AddBackward0>)\n",
            "tensor(0.0042, grad_fn=<AddBackward0>)\n",
            "tensor(0.0042, grad_fn=<AddBackward0>)\n",
            "tensor(0.0041, grad_fn=<AddBackward0>)\n",
            "tensor(0.0041, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0043, grad_fn=<AddBackward0>)\n",
            "tensor(0.0041, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0040, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0038, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0045, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0037, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msKFCQNCRpNW",
        "colab_type": "code",
        "outputId": "f25e21e6-8797-4b18-a737-4c4e47650234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hidden,cell = lstm.init_hidden_cell()\n",
        "\n",
        "for j in range(unroll_len-1):\n",
        "    input_data = one_hot[j:j+1].view(1,batch_size,hidden_size) \n",
        "    label = one_hot[j+1:j+1+1].view(1,batch_size,hidden_size) \n",
        "    \n",
        "    output, hidden, cell = rnn(input_data,hidden,cell)\n",
        "    print(onehot_to_word(output.data),end=\"\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello pytorch. how long can a rnn cell remember? show me your limit!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3J6r-FZ6X8r",
        "colab_type": "text"
      },
      "source": [
        "# Char_LSTM_Batch\n",
        "\n",
        "이번 코드는 batch first 코드로 작성함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkVR3dhxRrwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ4R_q4d8UtX",
        "colab_type": "code",
        "outputId": "9c1c1197-74a8-4399-f37b-45380ad12e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# string에 대한 preprocessing 적용\n",
        "# alphabet(0-25), space(26), start(27), end(28) -> 29 chars (0-28)\n",
        "\n",
        "string = \"hello pytorch. how long can a rnn cell remember? show me your limit!\"\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n",
        "char_list = list(chars)\n",
        "char_len = len(char_list)\n",
        "\n",
        "print(char_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X89KIKST8sjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# string을 one-hot vector로 바꾸어보자.\n",
        "# e.g. a -> [1, 0, ..., 0]\n",
        "\n",
        "def string_to_onehot(string):\n",
        "    start = np.zeros(shape=char_len, dtype=int)\n",
        "    end = np.zeros(shape=char_len, dtype=int)\n",
        "\n",
        "    start[-2] = 1\n",
        "    end[-1] = 1\n",
        "    \n",
        "    for i in string:\n",
        "        idx = char_list.index(i)\n",
        "        zero = np.zeros(shape=char_len, dtype=int)\n",
        "        zero[idx] = 1\n",
        "        \n",
        "        start = np.vstack([start, zero])\n",
        "    output = np.vstack([start, end])\n",
        "    \n",
        "    return output\n",
        "\n",
        "# onehot vector to word\n",
        "# [1, 0, ..., 0] -> a\n",
        "# 실제로는 inference 단계에서 수행해 주면 된다.\n",
        "\n",
        "def onehot_to_string(onehot):\n",
        "    output = onehot.numpy() # -> memory shared\n",
        "    # output = onehot.numpy().copy() # -> copy\n",
        "    return char_list[output.argmax()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wa5WwzR_-WJ",
        "colab_type": "code",
        "outputId": "76efee8e-9775-4682-b392-60f98875a68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Hyper-parameter 설정\n",
        "batch_size = 5\n",
        "\n",
        "seq_len = 1\n",
        "\n",
        "num_layers = 3\n",
        "input_size = char_len # Q. 왜 input size가 이거지?\n",
        "hidden_size = 35\n",
        "lr = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "\n",
        "print(one_hot.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 35])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao2IFD7YAa48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM with 1 hidden layer\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, intput_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, input_, hidden, cell):\n",
        "        # 여기서는 또 concat을 안하네?\n",
        "        output, (hidden, cell) = self.lstm(input_, (hidden, cell))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def init_hidden_cell(self):\n",
        "        hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "        cell = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "        return hidden, cell\n",
        "\n",
        "lstm = LSTM(input_size, hidden_size, num_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX1JrRanEX_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss and Optimizer\n",
        "loss_fnc = nn.MSELoss()\n",
        "optimizer = optim.Adam(lstm.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoQ5cZ-gEkiq",
        "colab_type": "code",
        "outputId": "56609ca7-05a0-439b-d093-7babc648916b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "j = 0\n",
        "# batch_first=True인 경우, input은 [batch, seq, input_size]를 만들어줌.\n",
        "input_data = one_hot[j:j+batch_size].view(batch_size, seq_len, input_size)\n",
        "print(one_hot[j:j+batch_size].size(), input_data.size()) # [70*35]를 batch_size=5만큼 잘랐으니, [5x35]에서 [5x1x35]로 바꾸어줌.\n",
        "\n",
        "# batch_first와 무관하게 [num_layers * num_directions, batch, hidden_size]가 됨.\n",
        "hidden, cell = lstm.init_hidden_cell()\n",
        "print(hidden.size(), cell.size()) \n",
        "\n",
        "# output은 input과 같은 shape을 갖음.\n",
        "output, hidden, cell = lstm(input_data, hidden, cell)\n",
        "print(output.size(), hidden.size(), cell.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 35]) torch.Size([5, 1, 35])\n",
            "torch.Size([3, 5, 35]) torch.Size([3, 5, 35])\n",
            "torch.Size([5, 1, 35]) torch.Size([3, 5, 35]) torch.Size([3, 5, 35])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9fIQpwBHMrq",
        "colab_type": "code",
        "outputId": "98866c14-9bc5-433d-90d5-2bebfb8244a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "unroll_len = one_hot.size()[0]//seq_len - 1 # 뭐여 이건 또.\n",
        "print(unroll_len)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    hidden, cell = lstm.init_hidden_cell()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for j in range(unroll_len - batch_size + 1):\n",
        "        # batch_size에 맞게 one_hot vector를 stack.\n",
        "        # e.g. batch_size=3이면, pytorch에서 pyt를 one-hot 벡터로 바꿔서 쌓고,\n",
        "        # 목표값으로 yto를 one-hot 벡터로 바꿔서 쌓는 과정이다.\n",
        "        input_data = torch.stack([one_hot[j+k:j+k+seq_len] for k in range(batch_size)], dim=0) # [5, 1, 35]\n",
        "        label = torch.stack([one_hot[j+k+1:j+k+seq_len+1] for k in range(batch_size)],dim=0)\n",
        "\n",
        "        output, hidden, cell = lstm(input_data, hidden, cell)\n",
        "        loss += loss_fnc(output.view(1, -1), label.view(1, -1)) # [1, 5 * 35]\n",
        "\n",
        "    # Q. batch 마다 gradient를 계산하는건가?\n",
        "    loss.backward() # loss함수는 mean으로 vector를 scalar로 바꾸어 줌.\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69\n",
            "tensor(2.3259, grad_fn=<AddBackward0>)\n",
            "tensor(1.7039, grad_fn=<AddBackward0>)\n",
            "tensor(1.6237, grad_fn=<AddBackward0>)\n",
            "tensor(1.4879, grad_fn=<AddBackward0>)\n",
            "tensor(1.3073, grad_fn=<AddBackward0>)\n",
            "tensor(0.9638, grad_fn=<AddBackward0>)\n",
            "tensor(0.5845, grad_fn=<AddBackward0>)\n",
            "tensor(0.3288, grad_fn=<AddBackward0>)\n",
            "tensor(0.1696, grad_fn=<AddBackward0>)\n",
            "tensor(0.0991, grad_fn=<AddBackward0>)\n",
            "tensor(0.0698, grad_fn=<AddBackward0>)\n",
            "tensor(0.0534, grad_fn=<AddBackward0>)\n",
            "tensor(0.0424, grad_fn=<AddBackward0>)\n",
            "tensor(0.0339, grad_fn=<AddBackward0>)\n",
            "tensor(0.0279, grad_fn=<AddBackward0>)\n",
            "tensor(0.0238, grad_fn=<AddBackward0>)\n",
            "tensor(0.0209, grad_fn=<AddBackward0>)\n",
            "tensor(0.0188, grad_fn=<AddBackward0>)\n",
            "tensor(0.0172, grad_fn=<AddBackward0>)\n",
            "tensor(0.0159, grad_fn=<AddBackward0>)\n",
            "tensor(0.0146, grad_fn=<AddBackward0>)\n",
            "tensor(0.0145, grad_fn=<AddBackward0>)\n",
            "tensor(0.0127, grad_fn=<AddBackward0>)\n",
            "tensor(0.0117, grad_fn=<AddBackward0>)\n",
            "tensor(0.0111, grad_fn=<AddBackward0>)\n",
            "tensor(0.0107, grad_fn=<AddBackward0>)\n",
            "tensor(0.0103, grad_fn=<AddBackward0>)\n",
            "tensor(0.0100, grad_fn=<AddBackward0>)\n",
            "tensor(0.0098, grad_fn=<AddBackward0>)\n",
            "tensor(0.0096, grad_fn=<AddBackward0>)\n",
            "tensor(0.0107, grad_fn=<AddBackward0>)\n",
            "tensor(0.0095, grad_fn=<AddBackward0>)\n",
            "tensor(0.0092, grad_fn=<AddBackward0>)\n",
            "tensor(0.0090, grad_fn=<AddBackward0>)\n",
            "tensor(0.0089, grad_fn=<AddBackward0>)\n",
            "tensor(0.0088, grad_fn=<AddBackward0>)\n",
            "tensor(0.0088, grad_fn=<AddBackward0>)\n",
            "tensor(0.0087, grad_fn=<AddBackward0>)\n",
            "tensor(0.0086, grad_fn=<AddBackward0>)\n",
            "tensor(0.0085, grad_fn=<AddBackward0>)\n",
            "tensor(0.0085, grad_fn=<AddBackward0>)\n",
            "tensor(0.0084, grad_fn=<AddBackward0>)\n",
            "tensor(0.0083, grad_fn=<AddBackward0>)\n",
            "tensor(0.0083, grad_fn=<AddBackward0>)\n",
            "tensor(0.0082, grad_fn=<AddBackward0>)\n",
            "tensor(0.0082, grad_fn=<AddBackward0>)\n",
            "tensor(0.0081, grad_fn=<AddBackward0>)\n",
            "tensor(0.0081, grad_fn=<AddBackward0>)\n",
            "tensor(0.0081, grad_fn=<AddBackward0>)\n",
            "tensor(0.0082, grad_fn=<AddBackward0>)\n",
            "tensor(0.0083, grad_fn=<AddBackward0>)\n",
            "tensor(0.0080, grad_fn=<AddBackward0>)\n",
            "tensor(0.0080, grad_fn=<AddBackward0>)\n",
            "tensor(0.0079, grad_fn=<AddBackward0>)\n",
            "tensor(0.0079, grad_fn=<AddBackward0>)\n",
            "tensor(0.0079, grad_fn=<AddBackward0>)\n",
            "tensor(0.0079, grad_fn=<AddBackward0>)\n",
            "tensor(0.0078, grad_fn=<AddBackward0>)\n",
            "tensor(0.0078, grad_fn=<AddBackward0>)\n",
            "tensor(0.0078, grad_fn=<AddBackward0>)\n",
            "tensor(0.0078, grad_fn=<AddBackward0>)\n",
            "tensor(0.0078, grad_fn=<AddBackward0>)\n",
            "tensor(0.0078, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0082, grad_fn=<AddBackward0>)\n",
            "tensor(0.0078, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0075, grad_fn=<AddBackward0>)\n",
            "tensor(0.0079, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rciklI6VOwRq",
        "colab_type": "code",
        "outputId": "fc47db52-1f6b-4f07-be7f-9540550c001b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for j in range(unroll_len - batch_size + 1):\n",
        "        # batch_size에 맞게 one_hot vector를 stack.\n",
        "        # e.g. batch_size=3이면, pytorch에서 pyt를 one-hot 벡터로 바꿔서 쌓고,\n",
        "        # 목표값으로 yto를 one-hot 벡터로 바꿔서 쌓는 과정이다.\n",
        "        input_data = torch.stack([one_hot[j+k:j+k+seq_len] for k in range(batch_size)], dim=0) # [5, 1, 35]\n",
        "        label = torch.stack([one_hot[j+k+1:j+k+seq_len+1] for k in range(batch_size)],dim=0)\n",
        "\n",
        "        print(\"\".join([onehot_to_string(i) for i in input_data]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0hell\n",
            "hello\n",
            "ello \n",
            "llo p\n",
            "lo py\n",
            "o pyt\n",
            " pyto\n",
            "pytor\n",
            "ytorc\n",
            "torch\n",
            "orch.\n",
            "rch. \n",
            "ch. h\n",
            "h. ho\n",
            ". how\n",
            " how \n",
            "how l\n",
            "ow lo\n",
            "w lon\n",
            " long\n",
            "long \n",
            "ong c\n",
            "ng ca\n",
            "g can\n",
            " can \n",
            "can a\n",
            "an a \n",
            "n a r\n",
            " a rn\n",
            "a rnn\n",
            " rnn \n",
            "rnn c\n",
            "nn ce\n",
            "n cel\n",
            " cell\n",
            "cell \n",
            "ell r\n",
            "ll re\n",
            "l rem\n",
            " reme\n",
            "remem\n",
            "ememb\n",
            "membe\n",
            "ember\n",
            "mber?\n",
            "ber? \n",
            "er? s\n",
            "r? sh\n",
            "? sho\n",
            " show\n",
            "show \n",
            "how m\n",
            "ow me\n",
            "w me \n",
            " me y\n",
            "me yo\n",
            "e you\n",
            " your\n",
            "your \n",
            "our l\n",
            "ur li\n",
            "r lim\n",
            " limi\n",
            "limit\n",
            "imit!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q85JbPKiPa-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}