{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Weight Regularization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/Pytorch-study/blob/master/7%EC%9E%A5%20%ED%95%99%EC%8A%B5%20%EC%8B%9C%20%EC%83%9D%EA%B8%B8%20%EC%88%98%20%EC%9E%88%EB%8A%94%20%EB%AC%B8%EC%A0%9C%EC%A0%90/2.%20Weight_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4m58em5mPjV",
        "colab_type": "text"
      },
      "source": [
        "# Weight Regularization\n",
        "- 최적화 함수의 weight decay로 강도를 조절할 수 있다\n",
        "- ```torch.optim.SGD(params, lr=1, momentum=0, dampening=0, weight_decay=0, nesterov=False)```\n",
        "- 모델이 오버피팅될 경우, 적절한 regularization을 통해 이를 극복할 수 있음.\n",
        "- 정형화 부분을 빼고는 CNN 코드와 동일함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsuMsm1CPjJk",
        "colab_type": "text"
      },
      "source": [
        "## Parameter Norm Penalties\n",
        "많은 regularization 방법은 parrameter norm penalty인 $\\Omega(\\theta)$를 objective function $J$에 추가하여 모델의 수용력을 제한시키는 방법에 기반한다. 이러한 regularized ojbective function을 $\\tilde J$를 다음과 같이 표현한다:\n",
        "$$\n",
        "\\tilde J(\\theta; X, y) = J(\\theta; X, y) + \\alpha\\Omega(\\theta)\n",
        "$$\n",
        "where $\\alpha \\in [0, \\infty)$  is a hyperparamter that weights the relative contribution of the norm penealty, $\\Omega$, relative to the standard objective funcion J.  \n",
        "이에 대해 더 자세하게 탐구하기 전에 한가지 알아야 할 것은 neural network에서 paramter norm penalty $\\Omega$는 각 layer에서 affine transformation의 *weight만을* penalize하고 bias는 regularize 하지 않는다는 점이다. Bias는 정확하게 fitting하기 위해 weight에 비해 덜 많은 데이터를 필요로 한다. 각 weight는 두 변수가 어떠한 관계가 있는지를 파악한다. 반면 각 bias는 오직 하나의 variable만을 컨트롤한다. 즉, 이는 biase는 unregularize한 상태로 내버려둔채로 너무 많은 variance를 유도하지 않는다는 뜻이다. 또한,  bias parameter를 regularizing하면 과도한 underfitting이 발생할 수 있다. 그러므로, vector $w$를 이용하여 norm penalty에 영향을 끼치는 weight를 표현하고, $\\theta$는 모든 parameter를 표현한다.  \n",
        "Neural network에서 각 layer마다 다른 coefficient $\\alpha$를 통해 분리된 penalty를 사용하는 것이 바람직하다. 그러나 이는 여러 hyperparameter의 정확한 값을 찾는데 소모되는 비용이 생기기 때문에, search space의 크기를 줄이기 위해서 같은 weight decay를 사용하는 것도 합리적인 선택이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8T7HUQtX1RG",
        "colab_type": "text"
      },
      "source": [
        "### 7.1.1 L2 Parameter Regularization\n",
        "L2 norm은 일반적으로 wegith decay라고도 알려져있다. 이 regularization 전략은 regularization term $\\Omega(\\theta)=\\frac1 2 ||w||^2_2$를 objective function에 더하여 weights를 원점(origin)에 가깝게 유도하는 것이다. 좀 더 간단하게 표현하면, bias parameter는 없다고 가정하므로, $\\theta$는 그냥 $w$가 된다. 이러한 모델은 다음과 같은 total objective function을 갖는다:\n",
        "$$\n",
        "\\tilde J(w; X, y) = \\frac\\alpha 2 w^Tw + J(w; X, y),\n",
        "$$  \n",
        "그리고 다음에 대응되는 parameter gradient를 갖는다  \n",
        "$$\n",
        "\\nabla_w\\tilde J(w; X, y) = \\alpha w + \\nabla_w J(w;X,y).\n",
        "$$  \n",
        "weight를 업데이트 하기 위해 하나의 gradient step만 취할 경우:  \n",
        "$$\n",
        "w \\gets w - \\epsilon(\\alpha w + \\nabla _w J(w; X, y)).\n",
        "$$  \n",
        "다른 방식으로 표현하면 업데이트는 다음과 같다:\n",
        "$$\n",
        "w \\gets (1-\\epsilon\\alpha)w - \\epsilon\\nabla_wJ(w; X, y).\n",
        "$$  \n",
        "\n",
        "보다시피 weight decay term의 덧셈이 learning rule을 수정하였다. 일반적인 gradient step을 수행하기전에, 매 step에서 constant factor($\\alpha$)의 곱셈을 통한 weight vector를 수렴시킨다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pcxD8JDO3Of",
        "colab_type": "text"
      },
      "source": [
        "# Regularization\n",
        "\"Any modification we make to a learning algorithm that is intended to **reduce its generalization error** but **not its training error**\"  \n",
        "\n",
        "Deep learning의 맥락에서, 대부분의 regularization 전략은 estimators를 regularizing 하는데 초점이 맞춰져 있다. 즉, bias를 높히고, variance를 낮추는 것이 그 목적이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnqaSnjknLXT",
        "colab_type": "text"
      },
      "source": [
        "# 1. Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPaK-PePnN0p",
        "colab_type": "text"
      },
      "source": [
        "## 1) Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3KpIMp3l9JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTa7bccmnjzB",
        "colab_type": "text"
      },
      "source": [
        "## 2) Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPL1t3n5njJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "learning_rate = 0.0002\n",
        "num_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwB7HwUkoOA3",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGGJ4swzoPSh",
        "colab_type": "code",
        "outputId": "d07f6522-3830-45df-fdfb-e9bf344f589f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Download data\n",
        "\n",
        "# transforms.ToTensor() -> PIL image나 numpy형식을 torch.tensor로 바꾸어줌.\n",
        "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "mnist_test = dset.MNIST(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 21003748.08it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 329694.18it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5346402.12it/s]                           \n",
            "8192it [00:00, 128560.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGRR_PLwuVNu",
        "colab_type": "code",
        "outputId": "c28097e2-542e-411c-bac7-5ab5935b693b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Check Dataset\n",
        "\n",
        "# 정보를 확인할 수 있음.\n",
        "print(mnist_train)\n",
        "print('='*100)\n",
        "\n",
        "# mnist_train의 element들은 data와 label의 tuple로 이루어져있고, 60000개가 있음.\n",
        "# print(mnist_train[0])\n",
        "\n",
        "# 데이터는 [1x28x28]\n",
        "print(mnist_train[0][0].size())\n",
        "# label은 integer\n",
        "print(mnist_test[0][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "====================================================================================================\n",
            "torch.Size([1, 28, 28])\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziMds1Os8mVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set dataloader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=False,num_workers=2,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j66W6u9KvzBX",
        "colab_type": "text"
      },
      "source": [
        "# 3. Model & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9vSA_hpu74e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            # CNN(input_channel, output_channel, kernel_size)\n",
        "            nn.Conv2d(1, 16, 3, padding=1), # 28 x 28 -> 30 x 30 (padding) -> 28 x 28\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), # 28 x 28 -> 30 x 30 (padding) -> 28 x 28\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # MaxPool2d(kernel_size, stride)\n",
        "            nn.MaxPool2d(2, 2), # 28 x 28 -> 14 x 14\n",
        "            nn.Conv2d(32, 64, 3, padding=1), # 14 x 14 -> 16 x 16 -> 14 x 14\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2) # 14 x 14 -> 7 x 7\n",
        "        )\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(7*7*64, 100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.layer(x)\n",
        "        output = output.view(batch_size, -1) # [batch_size x 7*7*64 x 100]\n",
        "        output = self.fc_layer(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJwpqSHP0Yrh",
        "colab_type": "code",
        "outputId": "960134b4-a863-4900-8c53-70fa156a5b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loss and Optimizer\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = CNN().to(device)\n",
        "loss_func = nn.CrossEntropyLoss() # Q. 6장 RNN에서는 CrossEntropy쓰면 에러가 났었는데...?\n",
        "\n",
        "# Regularization\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu6Ri-TP48RT",
        "colab_type": "code",
        "outputId": "21f8123a-4676-4741-dbe6-bdb4e315eebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Train\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    for j, [image, label] in enumerate(train_loader):\n",
        "        x = image.to(device)\n",
        "        y_ = label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(x)\n",
        "        loss = loss_func(output, y_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FzanGwL8ZlD",
        "colab_type": "code",
        "outputId": "ca0acf8a-e67c-42c3-e661-f03cd31f21f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "        x = image.to(device)\n",
        "        y_ = label.to(device)\n",
        "\n",
        "        output = model(x)\n",
        "        output_idx = torch.argmax(output, 1) # 아래와 같음. \n",
        "        # _,output_idx = torch.max(output,1)\n",
        "\n",
        "        total += label.size(0)\n",
        "        correct += (output_idx == y_).sum().float()\n",
        "\n",
        "    print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Test Data: 10.226362228393555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Vi1e41xsWF",
        "colab_type": "text"
      },
      "source": [
        "# Experiment\n",
        "L1 loss를 적용해보도록 하자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDBMBAVDxs_4",
        "colab_type": "code",
        "outputId": "389dd6f9-6a1a-490b-8aa4-ee32f6a17338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Loss and Optimizer\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model1 = CNN().to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model1.parameters(), lr=learning_rate)\n",
        "c = 1.0\n",
        "\n",
        "# Train\n",
        "for i in range(num_epochs):\n",
        "    for j, [image, label] in enumerate(train_loader):\n",
        "        x = image.to(device)\n",
        "        y_ = label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = loss_func(output, y_) # scalar임.\n",
        "\n",
        "        # loss = loss + L1_norm 이므로, L1norm을 구해서 더해줌.\n",
        "        norm = torch.tensor(0.0).to(device) # torch.cuda.FloatTensor()로 scalar 변형이 안되길래 이렇게 함.\n",
        "\n",
        "        # 그냥 data만 더할경우, parameter가 아니기 때문에, 학습이 제대로 진행되지 않는다.\n",
        "        # 따라서 parameters()를 통해서 parameter자체를 norm으로 더해준다.\n",
        "        for parameter in model1.parameters():\n",
        "            temp = torch.norm(parameter, p=1)\n",
        "            norm += temp\n",
        "        loss += c * norm\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "tensor(128.2100, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaELbK4C3SEf",
        "colab_type": "code",
        "outputId": "c5ffa2a1-7164-4cdf-fad2-98fc322c0fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "        x = image.to(device)\n",
        "        y_ = label.to(device)\n",
        "\n",
        "        output = model1(x)\n",
        "        output_idx = torch.argmax(output, 1) # 아래와 같음. \n",
        "        # _,output_idx = torch.max(output,1)\n",
        "\n",
        "        total += label.size(0)\n",
        "        correct += (output_idx == y_).sum().float()\n",
        "\n",
        "    print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Test Data: 9.805688858032227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHbPhZsX82pS",
        "colab_type": "text"
      },
      "source": [
        "## L2 norm도 똑같이 구해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46W8MF8g6hOT",
        "colab_type": "code",
        "outputId": "ec7f5dd4-da35-4672-b670-a14a00d1ed55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Loss and Optimizer\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model2 = CNN().to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model2.parameters(), lr=learning_rate)\n",
        "c = 0.1 # decay값이랑 동일하게 세팅해보자.\n",
        "\n",
        "# Train\n",
        "for i in range(num_epochs):\n",
        "    for j, [image, label] in enumerate(train_loader):\n",
        "        x = image.to(device)\n",
        "        y_ = label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = loss_func(output, y_) # scalar임.\n",
        "\n",
        "        # loss = loss + L2_norm 이므로, L2 norm을 구해서 더해주어야 함.\n",
        "        norm = torch.tensor(0.0).to(device) # torch.cuda.FloatTensor()로 scalar 변형이 안되길래 이렇게 함.\n",
        "\n",
        "        # 그냥 data만 더할경우, parameter가 아니기 때문에, 학습이 제대로 진행되지 않는다.\n",
        "        # 따라서 parameters()를 통해서 parameter자체를 norm으로 더해준다.\n",
        "        for parameter in model2.parameters():\n",
        "            temp = torch.norm(parameter, p=2)\n",
        "            norm += temp\n",
        "        loss += c * norm\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "tensor(4.2307, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0CvKD7_9O9W",
        "colab_type": "text"
      },
      "source": [
        " 기존의 optimizer에서 weight_decay로 넣었던 것보다 loss가 크다. 비슷하게 나올 것으로 기대했는데..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD_hJyi69HWq",
        "colab_type": "code",
        "outputId": "23c52117-c952-4281-c79e-d1a64de9ce0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "        x = image.to(device)\n",
        "        y_ = label.to(device)\n",
        "\n",
        "        output = model2(x)\n",
        "        output_idx = torch.argmax(output, 1) # 아래와 같음. \n",
        "        # _,output_idx = torch.max(output,1)\n",
        "\n",
        "        total += label.size(0)\n",
        "        correct += (output_idx == y_).sum().float()\n",
        "\n",
        "    print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Test Data: 9.815705299377441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcdZNNM79dq1",
        "colab_type": "text"
      },
      "source": [
        "정확도는 **L1 loss, L2 loss, Custom L2 loss** 모두 비슷한 것으로 확인 됨.  \n",
        "Weight를 한번 비교해보도록 하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZr31cLNIxAf",
        "colab_type": "code",
        "outputId": "cbb23568-18d0-436f-ab79-52d9e1f06750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "def check_weight(model, sparse_threshold=0.0005):\n",
        "    for param in model.parameters():\n",
        "        # nz = torch.where(param == 0) # weight가 0인 경우는 없었음.\n",
        "        t = torch.abs(param).max() * sparse_threshold # 따라서 적절한 threshold 밑에 있는 수가 몇인지를 파악해봄.\n",
        "        nz = torch.where(abs(param) < t)[0].size()[0]\n",
        "        print(nz)\n",
        "\n",
        "print('L2 norm')\n",
        "check_weight(model)\n",
        "print('L1 norm')\n",
        "check_weight(model1)\n",
        "print('Custom L2 norm')\n",
        "check_weight(model2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 norm\n",
            "0\n",
            "0\n",
            "3\n",
            "0\n",
            "15\n",
            "0\n",
            "154\n",
            "0\n",
            "0\n",
            "0\n",
            "L1 norm\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "5\n",
            "0\n",
            "171\n",
            "0\n",
            "1\n",
            "0\n",
            "Custom L2 norm\n",
            "0\n",
            "0\n",
            "4\n",
            "0\n",
            "16\n",
            "0\n",
            "138\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZvPTGooMKYE",
        "colab_type": "text"
      },
      "source": [
        "흠 애매하다... 확실하지 않으니 epochs를 늘려서 다시 한번 ㄱ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO2yg3v_MohT",
        "colab_type": "code",
        "outputId": "6eb31ff7-4daf-4941-feae-ad5fe83f2d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Loss and Optimizer\n",
        "num_epochs = 100 # 200으로 하면 너무 오래걸림.\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# 슬슬 귀찮음.\n",
        "model = CNN().to(device)\n",
        "model1 = CNN().to(device)\n",
        "model2 = CNN().to(device)\n",
        "models = [(model, None), (model1, 1.0), (model2, 0.1)] # model, p\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(\n",
        "    list(model.parameters())\n",
        "    + list(model1.parameters())\n",
        "    + list(model2.parameters()), lr=learning_rate)\n",
        "# c = 1.0 # 따로 넣어주겠음.\n",
        "\n",
        "def train(model, x, y_, c, p=None):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "    loss = loss_func(output, y_)\n",
        "\n",
        "    if p == None:\n",
        "        pass        \n",
        "    else:\n",
        "        for parameter in model.parameters():\n",
        "            temp = torch.norm(parameter, p=p)\n",
        "            norm += temp\n",
        "\n",
        "        loss += c * norm\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Train\n",
        "for i in range(num_epochs):\n",
        "    for j, [image, label] in enumerate(train_loader):\n",
        "        x = image.to(device)\n",
        "        y_ = label.to(device)\n",
        "        \n",
        "        # 깔끔한 코드를 위해 함수화\n",
        "        for model, p in models:\n",
        "            train(model, x, y_, p)\n",
        "    if i % 10 == 0:\n",
        "        print(i )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L26thVMSaimm",
        "colab_type": "code",
        "outputId": "3cf3c1f9-6e60-4e50-e4d2-d48df9aeea51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Test\n",
        "\n",
        "with torch.no_grad():\n",
        "    for model, _ in models:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for image, label in test_loader:\n",
        "            x = image.to(device)\n",
        "            y_ = label.to(device)\n",
        "\n",
        "            output = model(x)\n",
        "            output_idx = torch.argmax(output, 1) # 아래와 같음. \n",
        "            # _,output_idx = torch.max(output,1)\n",
        "\n",
        "            total += label.size(0)\n",
        "            correct += (output_idx == y_).sum().float()\n",
        "\n",
        "        print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Test Data: 59.675479888916016\n",
            "Accuracy of Test Data: 42.72836685180664\n",
            "Accuracy of Test Data: 59.45512771606445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdXNhA8PnGCh",
        "colab_type": "text"
      },
      "source": [
        "음 어큐러시가 상당히 올라갔네"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ZREzWEnHaY",
        "colab_type": "code",
        "outputId": "1923e144-2ba9-44fa-86d6-1cfd7245e69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "def check_weight(model, sparse_threshold=0.0005):\n",
        "    for param in model.parameters():\n",
        "        # nz = torch.where(param == 0) # weight가 0인 경우는 없었음.\n",
        "        t = torch.abs(param).max() * sparse_threshold # 따라서 적절한 threshold 밑에 있는 수가 몇인지를 파악해봄.\n",
        "        nz = torch.where(abs(param) < t)[0].size()[0]\n",
        "        print(nz)\n",
        "\n",
        "print('L2 norm')\n",
        "check_weight(model)\n",
        "print('L1 norm')\n",
        "check_weight(model1)\n",
        "print('Custom L2 norm')\n",
        "check_weight(model2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 norm\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "10\n",
            "0\n",
            "230\n",
            "0\n",
            "0\n",
            "0\n",
            "L1 norm\n",
            "0\n",
            "0\n",
            "7\n",
            "0\n",
            "14\n",
            "0\n",
            "256\n",
            "0\n",
            "0\n",
            "0\n",
            "Custom L2 norm\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "10\n",
            "0\n",
            "230\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIXx9nomnpO0",
        "colab_type": "text"
      },
      "source": [
        "생각만큼 의미있지는 않은듯. 여튼 뭐 줄어드는 것은 확인하였고, custom L2 loss와 동일함을 확인함."
      ]
    }
  ]
}