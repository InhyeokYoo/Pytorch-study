{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6. Learning Rate Decay.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/Pytorch-study/blob/master/Ch%207.%20%ED%95%99%EC%8A%B5%20%EC%8B%9C%20%EC%83%9D%EA%B8%B8%20%EC%88%98%20%EC%9E%88%EB%8A%94%20%EB%AC%B8%EC%A0%9C/%206.%20Learning_Rate_Decay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaVmFNyZ93MW",
        "colab_type": "text"
      },
      "source": [
        "# 학습률 스케줄러 Learning Rate Scheduler\n",
        "- optim.lr_schedular.StepLR()\n",
        "- optim.lr_schedular.MultiStepLR()\n",
        "- optim.lr_schedular.ExponentialLR()\n",
        "- optim.lr_schedular.ReduceLROnPlateau()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u7LUf5V-O7b",
        "colab_type": "text"
      },
      "source": [
        "# 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-FlHeSO9yIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.optim import lr_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsqhhVWe-SBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper parameters\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "num_epoch = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DUXUVQp-e0m",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1sx52PD-c4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "mnist_test = dset.MNIST(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfNo2u_fCbrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=False,num_workers=2,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN_hxwK2CdXk",
        "colab_type": "text"
      },
      "source": [
        "# 3. Model & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKhtwohYCcrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN,self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(1,16,3,padding=1),  # 28 x 28\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16,32,3,padding=1), # 28 x 28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),            # 14 x 14\n",
        "            nn.Conv2d(32,64,3,padding=1), # 14 x 14\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)             #  7 x 7\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64*7*7,100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100,10)\n",
        "        )        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        out = self.layer(x)\n",
        "        out = out.view(batch_size,-1)\n",
        "        out = self.fc_layer(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n272SDGCgN6",
        "colab_type": "code",
        "outputId": "12b3f66b-80e7-43f6-a479-a88e6270e30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = CNN().to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 지정한 스텝 단위로 학습률에 감마를 곱해 학습률을 감소시킵니다.\n",
        "#scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma= 0.99)       \n",
        "\n",
        "# 지정한 스텝 지점(예시에서는 10,30,80)마다 학습률에 감마를 곱해줍니다.\n",
        "#scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10,30,80], gamma= 0.1)  \n",
        "\n",
        "# 매 epoch마다 학습률에 감마를 곱해줍니다.\n",
        "#scheduler = lr_scheduler.ExponentialLR(optimizer, gamma= 0.99)                             \n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html?highlight=lr_scheduler#torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "# 지정한 메트릭으로 측정한 값이 더 나아지지 않으면 학습률을 감소시킵니다. ex) 정확도, dice score 등등\n",
        "# 이 스케쥴러에는 다양한 인자가 들어가는데 각각의 역할은 도큐먼트를 참고 바랍니다.\n",
        "# 여기서는 patience 즉, 지정한 값이 줄어들지 않을때 몇 epoch 만큼을 지켜볼 것인지를 1로 낮춰놨기 때문에 매 epoch 마다 학습률이 감소하는것을 확인할 수 있습니다.\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,threshold=1,patience=1,mode='min')    \n",
        "\n",
        "# 참고 https://www.geeksforgeeks.org/python-dir-function/\n",
        "print(dir(scheduler))\n",
        "print(dir(optimizer))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_init_is_better', '_reduce_lr', '_reset', 'best', 'cooldown', 'cooldown_counter', 'eps', 'factor', 'in_cooldown', 'is_better', 'last_epoch', 'load_state_dict', 'min_lrs', 'mode', 'mode_worse', 'num_bad_epochs', 'optimizer', 'patience', 'state_dict', 'step', 'threshold', 'threshold_mode', 'verbose']\n",
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add_param_group', 'defaults', 'load_state_dict', 'param_groups', 'state', 'state_dict', 'step', 'zero_grad']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzzP8Mf3Oyk1",
        "colab_type": "code",
        "outputId": "38d2066b-6137-4a69-a2c4-95e2d0a0af86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "for i in range(num_epoch):\n",
        "    # ReduceLRONPlateau 빼고는 아래의 코드를 사용하세요\n",
        "    #scheduler.step()  \n",
        "    for j,[image,label] in enumerate(train_loader):\n",
        "        x = image.to(device)\n",
        "        y_= label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(x)\n",
        "        loss = loss_func(output,y_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # ReduceLRONPlateau 만 해당됩니다. 이 코드에서는 손실이 줄어들지 않으면 학습률을 낮추도록 만들어놨습니다.\n",
        "    scheduler.step(loss)      \n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print(loss)   \n",
        "            \n",
        "    #print(\"Epoch: {}, Learning Rate: {}\".format(i,scheduler.get_lr()))  \n",
        "    print(\"Epoch: {}, Learning Rate: {}\".format(i,scheduler.optimizer.state_dict()['param_groups'][0]['lr']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Epoch: 0, Learning Rate: 0.001\n",
            "Epoch: 1, Learning Rate: 0.0001\n",
            "Epoch: 2, Learning Rate: 0.0001\n",
            "Epoch: 3, Learning Rate: 1e-05\n",
            "Epoch: 4, Learning Rate: 1e-05\n",
            "Epoch: 5, Learning Rate: 1.0000000000000002e-06\n",
            "Epoch: 6, Learning Rate: 1.0000000000000002e-06\n",
            "Epoch: 7, Learning Rate: 1.0000000000000002e-07\n",
            "Epoch: 8, Learning Rate: 1.0000000000000002e-07\n",
            "Epoch: 9, Learning Rate: 1.0000000000000004e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aJ2VIrIO0G2",
        "colab_type": "code",
        "outputId": "162f706b-e82e-49d0-a30a-db5740da91a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for image,label in test_loader:\n",
        "      x = image.to(device)\n",
        "      y_= label.to(device)\n",
        "\n",
        "      output = model.forward(x)\n",
        "      _,output_index = torch.max(output,1)\n",
        "\n",
        "      total += label.size(0)\n",
        "      correct += (output_index == y_).sum().float()\n",
        "\n",
        "  print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Test Data: 9.755609512329102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xhp8B7sO1kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}