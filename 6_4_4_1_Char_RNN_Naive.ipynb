{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.4.4.1 Char_RNN_Naive.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InhyeokYoo/Pytorch-study/blob/master/6_4_4_1_Char_RNN_Naive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymgVATJ2aaqB",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI5b2jhJaEa-",
        "colab_type": "code",
        "outputId": "cd8f9a30-6c1d-4389-fa8a-d7fd974a37b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!rm -r data\n",
        "import os \n",
        "\n",
        "try:\n",
        "  os.mkdir(\"./data\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "!wget https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt -P ./data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-29 07:45:08--  https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘./data/input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-29 07:45:09 (43.1 MB/s) - ‘./data/input.txt’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBivfBPnaOVv",
        "colab_type": "code",
        "outputId": "5e144b3d-7bf4-46e0-9aa5-c619c7661e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4vllVHaadfV",
        "colab_type": "text"
      },
      "source": [
        "## 1) Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-S4M-dXaSoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2VgcJ_Eahot",
        "colab_type": "code",
        "outputId": "c6c7ed70-ee68-4a92-d487-da6a565a76b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGbGeCqIajLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import time, math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev2vHuHWYJ0R",
        "colab_type": "text"
      },
      "source": [
        "## 2) Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX6YyP_qapQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "\n",
        "chunk_len = 200\n",
        "\n",
        "hidden_size = 100\n",
        "batch_size = 1\n",
        "num_layers = 1\n",
        "embedding_size = 70\n",
        "lr = 0.002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvUWgNqVYaYO",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbyn0yp9Yn6_",
        "colab_type": "text"
      },
      "source": [
        "## 1) Prepare characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezg8fTuHYY8X",
        "colab_type": "code",
        "outputId": "ed6f5d9a-ad62-436b-de33-cd0fb46b5a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# string module에서 출력 가능한 문자를 모두 불러오자.\n",
        "all_characters = string.printable\n",
        "print(all_characters)\n",
        "\n",
        "# 출력가능한 문자들의 개수를 저장해놓는다.\n",
        "n_characters = len(all_characters)\n",
        "print(n_characters)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHcn1OqZFTs",
        "colab_type": "text"
      },
      "source": [
        "## 2) Get Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PrYRuxkY42R",
        "colab_type": "code",
        "outputId": "caa8f2fb-6727-4f7d-a590-55629a14818f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 앞서 다운받은 텍스트 파일을 열어준다.\n",
        "file = unidecode.unidecode(open('./data/input.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len', file_len)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8kHEQsTZcqT",
        "colab_type": "text"
      },
      "source": [
        "# 3. functions for text processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON5wedJraGUF",
        "colab_type": "text"
      },
      "source": [
        "## 1) Random chunk\n",
        "Q. 왜 필요한건지 이해가 되지 않음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0ZGyNWTZWbO",
        "colab_type": "code",
        "outputId": "c778fc9c-1f92-4dc4-bca7-fa2efce0da59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# 텍스트 파일의 일부를 불러오는 함수\n",
        "def random_chunk():\n",
        "    # (시작지점 < 텍스트 전체 길이 - 불러오는 길이)\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in some vantage.\n",
            "But make you ready your stiff bats and clubs:\n",
            "Rome and her rats are at the point of battle;\n",
            "The one side must have bale.\n",
            "Hail, noble Marcius!\n",
            "\n",
            "MARCIUS:\n",
            "Thanks. What's the matter, you d\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEej46x9aDIv",
        "colab_type": "text"
      },
      "source": [
        "## 2). char to tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FIAWoX9Z7JZ",
        "colab_type": "code",
        "outputId": "ab9583a0-04f6-4283-d3ef-dcc9c69ec3f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 문자열을 받았을 때, 이를 indices로 바꾸어서 return함.\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return tensor\n",
        "\n",
        "print(char_tensor('ABCdef'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([36, 37, 38, 13, 14, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y3rgrvsa2qj",
        "colab_type": "text"
      },
      "source": [
        "## 3) chunk into input & label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d76TgO-ta0tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 인덱스화된 문자열을 입력값과 목표값으로 나눠주는 함수\n",
        "# pytorch: pytorc -> ytorch\n",
        "\n",
        "def random_training_set():\n",
        "    chunk = random_chunk() # random하게 잘린 문자열\n",
        "    input_ = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return input_, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQrRbcH8bgZW",
        "colab_type": "text"
      },
      "source": [
        "# 3. Model & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq6mySXxbfN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size # 실제 input의 사이즈가 아니라, embedding lookup table의 개수.\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input_, hidden):\n",
        "        output = input_.view(1, -1) # [1 x 1]\n",
        "        \n",
        "        # embedding에선 indices들을 전달한다\n",
        "        # e.g. torch.LongTensor([[1,2,4,5],[4,3,2,9]]) -> 2개의 batch, [1, 2, 4, 5] 번째 indices와 [4, 3, 2, 9]의 indices\n",
        "        output = self.encoder(output) # [bacth x 1 x embedding_size]\n",
        "        # print('after emgedding', output.size())\n",
        "        \n",
        "        # hidden: [num_layers * num_directions, batch, hidden_size]\n",
        "        # output: [seq_len, batch, num_directions * hidden_size]\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        # print('before view output:', output.size())\n",
        "        # print('after decoder output:', output.view(batch_size, -1).size())\n",
        "        output = self.decoder(output.view(batch_size, -1))\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return hidden\n",
        "\n",
        "model = RNN(n_characters, embedding_size, hidden_size, n_characters, num_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eixhONFH3str",
        "colab_type": "text"
      },
      "source": [
        "```torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)```  \n",
        "A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "\n",
        "This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThZVyHZXRnuv",
        "colab_type": "code",
        "outputId": "7ed38824-49a4-4296-afc7-a456b32b6fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Embedding 연습\n",
        "\n",
        "embedding = nn.Embedding(10, 3)\n",
        "\n",
        "# a batch of 2 samples of 4 indices each\n",
        "indices = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) # 2 x 4\n",
        "\n",
        "embedding(indices) # [2 x 4 x 3] -> [(batch_size, indices), embedding_dim]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2434, 0.0799, 0.0629]], grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj-2IzVuKzqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(input_size=n_characters, \n",
        "            embedding_size=embedding_size,\n",
        "            hidden_size=hidden_size, \n",
        "            output_size=n_characters, \n",
        "            num_layers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGuJ8QXzidzj",
        "colab_type": "code",
        "outputId": "77f36fd0-1429-495d-938b-5032b084b8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# 모델 테스트\n",
        "\n",
        "inp = char_tensor(\"A\")\n",
        "print(inp, inp.size())\n",
        "print('View 적용:', inp.view(1, -1), inp.view(1, -1).size())\n",
        "\n",
        "embedding = nn.Embedding(100, 100)\n",
        "word_vector = embedding(inp.view(1, -1))\n",
        "print('after embedding', word_vector.size())\n",
        "\n",
        "hidden = model.init_hidden()\n",
        "print(hidden.size())\n",
        "out, hidden = model(inp, hidden)\n",
        "print(out.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([36]) torch.Size([1])\n",
            "View 적용: tensor([[36]]) torch.Size([1, 1])\n",
            "after embedding torch.Size([1, 1, 100])\n",
            "torch.Size([2, 1, 100])\n",
            "before view output: torch.Size([1, 1, 100])\n",
            "after decoder output: torch.Size([1, 100])\n",
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9peY88Xkkyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss() # 여긴 시발 또 cross entropy네?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRS7T-5yk75t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test function\n",
        "# 임의의 문자 start_str 로 시작하는 길이 200짜리 모방 글을 생성하는 예제\n",
        "\n",
        "def test():\n",
        "    start_str = 'b'\n",
        "    inp = char_tensor(start_str)\n",
        "    hidden = model.init_hidden()\n",
        "    x = inp\n",
        "\n",
        "    print(start_str, end='')\n",
        "\n",
        "    for i in range(200):\n",
        "        output, hidden = model(x, hidden)\n",
        "\n",
        "        # 여기서는 max값을 쓰지 않고 multinomial을 사용하는 이유는 만약 max값만 쓰는 경우에\n",
        "        # 생성되는 text가 the the the ... 만 나오기 때문임.\n",
        "        # multinomial 함수를 통해 높은 값을 가지는 문자들에 대해서 랜덤하게 다음 글자를 뽑아내는 방식으로 텍스트를 생성해보자\n",
        "\n",
        "        output_dist = output.data.view(-1).div(0.8).exp() # Q. 굳이 이렇게 한 이유는?\n",
        "\n",
        "        # input은 sample이고, 얼마나 뽑을지 결정하면 됨.\n",
        "        top_i = torch.multinomial(output_dist, 1)[0] # [0]은 scalar값만 가져오겠단 소리.\n",
        "        predicted_char = all_characters[top_i]\n",
        "\n",
        "        print(predicted_char, end=\"\")\n",
        "\n",
        "        x = char_tensor(predicted_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psydzwzh-g9o",
        "colab_type": "code",
        "outputId": "41c05d58-96f4-436a-e254-bbb8da6f29cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train\n",
        "# model = RNN(n_characters, embedding_size, hidden_size, n_characters, num_layers)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # 랜덤한 텍스트 덩어리를 sampling하고, 이를 index tensor로 변환한다.\n",
        "    input_, label = random_training_set() # 200개 짜리 chunk된 word의 indices\n",
        "    hidden = model.init_hidden()\n",
        "    \n",
        "    # Q. 왜 굳이 tensor로 loss를 받는가?\n",
        "    loss = torch.tensor([0]).type(torch.FloatTensor) # loss = 0 이라도 잘 작동함.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for j in range(chunk_len - 1):\n",
        "        x = input_[j] # 글자 하나의 index -> tensor(22)\n",
        "\n",
        "        # torch.unsqueeze(input, dim) -> 특정 자리에 차원을 추가하는 함수.\n",
        "        y_ = label[j].unsqueeze(0).type(torch.LongTensor) # -> tensor([22]), 1x1\n",
        "        y, hidden = model(x, hidden)\n",
        "        loss += loss_func(y, y_)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 100 ==0:\n",
        "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
        "        test()\n",
        "        print(\"\\n\",\"=\"*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " tensor(4.5962, grad_fn=<DivBackward0>) \n",
            "\n",
            "bkfwXle%Nh>#45L,]kX/ZN (Oz}4HLt^esHOZ\u000b?N#ORPlr@?n=GLAn=vt/#l8nED-j_97Q\"AZZH@U<1P|gSw$KU1:i@yY]'8c4q|cji/H<%'@Jd\":}5cEdIf+.TKfgD~[).7;d|^o\\YL~[WYQNvYr3Dw~\"\t]r\u000b\n",
            ">)7l<UTJN[Cy:OqLDx4uf}cc0tG9/qd\"!X9-1;f\u000b%}\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.3547, grad_fn=<DivBackward0>) \n",
            "\n",
            "bJ\n",
            "Tovin\n",
            "War sou sor, there aste pafes, abt uond,;\n",
            "\n",
            "O min mawis tarino to the whes fo ga; wot. tareey wiag the the thoud Wd lL roul thor whis,\n",
            "That yith th shers fallin roy eins ,ow thes wile seor ther\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.2918, grad_fn=<DivBackward0>) \n",
            "\n",
            "bure an me withee zece norfes and thet in of' tire Hot tim lo wing Mell sis will thel\n",
            "\n",
            "CD'''s mell not they ourt; beeat yite! mris maew\n",
            "\n",
            "Sowat yarincere fou raos thov, lich ther st.\n",
            "LE:\n",
            "Act nof I EH:\n",
            "T\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.2984, grad_fn=<DivBackward0>) \n",
            "\n",
            "been the aus, the cance of, sungust blise bulla gheon the mase the shas waly suy and\n",
            "Thenere,\n",
            "I shame jour noce be fom hand thear hak semall I thou hill ongy way thour whaan shell\n",
            "Hirs aw and for hake,\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.0571, grad_fn=<DivBackward0>) \n",
            "\n",
            "benther thour thy go; rectill nowerd sicing, to I this mall is your deser and yanted your marterse, the be of hith hall the of wackon raterow to me lreatnes in and\n",
            "My Of how you En thenerenuo,\n",
            "As me my\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.1530, grad_fn=<DivBackward0>) \n",
            "\n",
            "beed, the bir\n",
            "He beime panlished orshince bew in of ishe for theal the witen: sick lipe ce our lobrede of me he blich yaur nother thy the here Hechercand byat and bot upan\n",
            "to beef of sonet, notre is hi\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.2009, grad_fn=<DivBackward0>) \n",
            "\n",
            "bear'd hercand;\n",
            "Whis that ate,\n",
            "Whoth hange, not ster sout atem's wiqgoid there, this briin:\n",
            "And but brist love to were me mirone fill is good the to mace spaccarn, the wace is for ros mest to rime upor\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.0555, grad_fn=<DivBackward0>) \n",
            "\n",
            "but me thiu the way us.\n",
            "A'd a peath bettert lake wise whome, whot thou his lecerqunthan deall you then the chim with then in be sirexs mave the noth me malt that have hourel by prove whon fach,\n",
            "That an\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.8761, grad_fn=<DivBackward0>) \n",
            "\n",
            "bles: and not marlounlo;\n",
            "What to mave with rising,\n",
            "Wates, hath the marry my now chall math nalady my dage saing our cave, no digh, I bees our his not my moreds swankon, wartor,\n",
            "To your more stheres?\n",
            "\n",
            "Y\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.7241, grad_fn=<DivBackward0>) \n",
            "\n",
            "buch seing of the sing thee, as the raye's is manite the rave, in a be in with is and the be come and of mimes\n",
            "To and thinat, whill not be me he mast.\n",
            "\n",
            "AUTIO:\n",
            "Low ne dear, all Romsee,\n",
            "I conour gike,\n",
            "Yo\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.2297, grad_fn=<DivBackward0>) \n",
            "\n",
            "by a what the of my rod denence in the pightend,\n",
            "The I and stood me is thy fatherforder?\n",
            "\n",
            "GaMere fime, her of then dellany:\n",
            "I dadus self theigattune what his furdow of sumest in son't\n",
            "manterserth not h\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.0889, grad_fn=<DivBackward0>) \n",
            "\n",
            "bart nin.\n",
            "Whif, for the sury be our for mones, and with me, here\n",
            "The mare hear their reating mosher ridest hall teety more beed'd hence with dow kear the for no ale they\n",
            "by the to cerfige; and in't\n",
            "Sel\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(2.0619, grad_fn=<DivBackward0>) \n",
            "\n",
            "ble te no call your and a belliin\n",
            "your fanks.\n",
            "\n",
            "PLOUCENTAO:\n",
            "A alaged here ondear of this pore word and their no. What!\n",
            "\n",
            "GLUCELIO:\n",
            "And not thing her,\n",
            "But a woughture of at I'll you,\n",
            "The aldry\n",
            "us the blan\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.7669, grad_fn=<DivBackward0>) \n",
            "\n",
            "by if a gited and my must,\n",
            "And thou be with me and come, blentle that you hood be ofard the cention have send the prishis with you wank prich cate your be with to lince to lend'd, that well in thew coo\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.5895, grad_fn=<DivBackward0>) \n",
            "\n",
            "bren, and tell to hesper'd pack love lavant:\n",
            "Herefime,\n",
            "And Ongore couls\n",
            "and ow and she ore have have whis,\n",
            "And my plant's\n",
            "To knot sperder, be eecile ever her of have have is beitner,\n",
            "A prisifer\n",
            "To gire\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.9095, grad_fn=<DivBackward0>) \n",
            "\n",
            "bliad to my lovates!\n",
            "\n",
            "QUCIINIO:\n",
            "My thil, enour stand that,\n",
            "To not same live sain this my more arciest\n",
            "Marron thee know in the ride to Rinelenter are to my make coull ny abless;\n",
            "What partly him, to shal\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.9035, grad_fn=<DivBackward0>) \n",
            "\n",
            "by for mand that noted.\n",
            "\n",
            "ThARD:\n",
            "For horrow en,\n",
            "For his this no Bucko.\n",
            "\n",
            "SSPERAT:\n",
            "He you,\n",
            "To my for thy for the see lord,\n",
            "All faster and from then you mandy I song them Vound warge, harstignatime,\n",
            "So thi\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.6205, grad_fn=<DivBackward0>) \n",
            "\n",
            "bet,\n",
            "That the seny more to to for her stence thou arsaman be more, wow I and he bolly with have nost that for henger a bore bide pofor:\n",
            "Po ext for deaton a bight, we dears I hear a will teten, and, a n\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.7374, grad_fn=<DivBackward0>) \n",
            "\n",
            "bet sir, counted his not, good, And mean of my ploods you which you heap this mear come hindess tell the cens her his gate hold heen our lading hor thou fhace, like weart\n",
            "indedse! thy loup onour will f\n",
            " ====================================================================================================\n",
            "\n",
            " tensor(1.6873, grad_fn=<DivBackward0>) \n",
            "\n",
            "by to to thy grope, will by whin by lears:\n",
            "\n",
            "Byop thea meres is to chad the wright the crove ut in the better a trracker how him.\n",
            "\n",
            "ROMEO:\n",
            "It uper, in her I will supbay the made thy kish the watis hank t\n",
            " ====================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2HvJsrWmQDX",
        "colab_type": "text"
      },
      "source": [
        "# Experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfmHywoLmSu9",
        "colab_type": "text"
      },
      "source": [
        "## batch_size가 여러개인 RNN 모델을 직접 짜보자.\n",
        "\n",
        "사실 모델 내에서 batch 부분에 대해서 건들 것이 없음. 기존의 [input] -> [output] 구조를, [batch x input] -> [batch x output] 으로 바꿔주면 됨."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt4L66s1mdSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Hyper parameters\n",
        "num_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "chunk_len = 200\n",
        "hidden_size = 100\n",
        "batch_size = 5\n",
        "num_layers = 2\n",
        "embedding_size = 70\n",
        "lr = 0.002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMaH2vxbmOgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNBatch(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Architecture\n",
        "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input_, hidden):\n",
        "        # input: [batch] == [5]\n",
        "\n",
        "        # Embedding\n",
        "        # input: [batch x indices] == [5 x 1] 우리는 단어마다 받을 것 (seq_len=1)\n",
        "        output = self.embedding(input_.view(batch_size, -1))\n",
        "        # output: [batch x indices x dim] == [5 x 1 x 100]\n",
        "\n",
        "        # RNN\n",
        "        # input: [batch, seq_len, features] == [5 x 1 x 100]. seq_len=1 이고 embedding_size가 input dim\n",
        "        # hidden: [num_layers * num_directions, batch, hidden_Size] == [2, 5, 100]\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        # output: [batch, seq_len, features] == [5, 1, 100]. input과 같음.\n",
        "        # hidden: [num_layers * num_directions, batch, hidden_size] == [2, 5, 100]\n",
        "\n",
        "        # FC\n",
        "        # input: [batch_size, in_features] = [5 x 100]\n",
        "        output = output.view(batch_size, -1) # [2, 5, 100] -> [5, 100]\n",
        "        output = self.fc(output)\n",
        "        # output: [batch_size, output_size]\n",
        "        output = nn.functional.log_softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.num_layers * 1, batch_size, self.hidden_size)\n",
        "        return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOTiFaYZv-p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNNBatch(input_size=n_characters,\n",
        "          embedding_size=embedding_size,\n",
        "          hidden_size=hidden_size,\n",
        "          output_size=n_characters,\n",
        "          num_layers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x3lldA9t-6NG",
        "colab": {}
      },
      "source": [
        "# test function\n",
        "# 임의의 문자 start_str 로 시작하는 길이 200짜리 모방 글을 생성하는 예제\n",
        "\n",
        "def test_with_batch():\n",
        "    start_str = 'bbbbb'\n",
        "    inp = char_tensor(start_str) # [x, x, x, .. , x]\n",
        "    hidden = model.init_hidden()\n",
        "    x = inp\n",
        "    predicted_chars = ['' for _ in range(batch_size)]\n",
        "    print(start_str, end='')\n",
        "\n",
        "    for i in range(200):\n",
        "        output, hidden = model(x, hidden)\n",
        "\n",
        "        # 여기서는 max값을 쓰지 않고 multinomial을 사용하는 이유는 만약 max값만 쓰는 경우에\n",
        "        # 생성되는 text가 the the the ... 만 나오기 때문임.\n",
        "        # multinomial 함수를 통해 높은 값을 가지는 문자들에 대해서 랜덤하게 다음 글자를 뽑아내는 방식으로 텍스트를 생성해보자\n",
        "\n",
        "        output_dist = output.data.div(0.8).exp() \n",
        "        top_i = torch.multinomial(output_dist, 1).view(-1)\n",
        "        predicted_char = [all_characters[item] for item in top_i]\n",
        "        predicted_chars = [predicted_chars[item] + predicted_char[item] for item in range(batch_size)]\n",
        "\n",
        "\n",
        "        x = char_tensor(\"\".join(predicted_char))\n",
        "    \n",
        "    for item in predicted_chars:\n",
        "        print(item, sep='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34awPGucwjQN",
        "colab_type": "code",
        "outputId": "cb1de4fa-308a-4f0a-e9e6-17010752e624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "# 모델 테스트\n",
        "\n",
        "inp = char_tensor(\"ABCDE\")\n",
        "print(inp, inp.size())\n",
        "print('View 적용:', inp.view(1, -1), inp.view(batch_size, -1).size())\n",
        "\n",
        "embedding = nn.Embedding(100, 70)\n",
        "word_vector = embedding(inp.view(batch_size, -1))\n",
        "print('after embedding', word_vector.size())\n",
        "\n",
        "hidden = model.init_hidden()\n",
        "print(hidden.size())\n",
        "out, hidden = model(inp, hidden)\n",
        "print(out.size())\n",
        "\n",
        "# test 함수\n",
        "# output_dist = out.data.view(-1).div(0.8).exp()\n",
        "output_dist = out.data.div(0.8).exp()  # batch의 경우, view를 안해줘야 되니까.\n",
        "print(output_dist.size())\n",
        "\n",
        "print('='*100)\n",
        "print('multinomial')\n",
        "top_i = torch.multinomial(output_dist, 1)\n",
        "print(top_i, top_i.size())\n",
        "top_i = top_i.view(-1)\n",
        "print(top_i)\n",
        "predicted_char = [all_characters[item] for item in top_i]\n",
        "print(predicted_char)\n",
        "predicted_chars = ['' for _ in range(batch_size)]\n",
        "predicted_chars = [predicted_chars[item] + predicted_char[item] for item in range(batch_size)]\n",
        "print(predicted_chars)\n",
        "x = char_tensor(\"\".join(predicted_char))\n",
        "print(x)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([36, 37, 38, 39, 40]) torch.Size([5])\n",
            "View 적용: tensor([[36, 37, 38, 39, 40]]) torch.Size([5, 1])\n",
            "after embedding torch.Size([5, 1, 70])\n",
            "torch.Size([2, 5, 100])\n",
            "torch.Size([5, 100])\n",
            "torch.Size([5, 100])\n",
            "====================================================================================================\n",
            "multinomial\n",
            "tensor([[13],\n",
            "        [47],\n",
            "        [48],\n",
            "        [23],\n",
            "        [77]]) torch.Size([5, 1])\n",
            "tensor([13, 47, 48, 23, 77])\n",
            "['d', 'L', 'M', 'n', ':']\n",
            "['d', 'L', 'M', 'n', ':']\n",
            "tensor([13, 47, 48, 23, 77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNbsE-lSxB2I",
        "colab_type": "text"
      },
      "source": [
        "잘 나오는 것을 확인함. 그러나 문제는 training하는 과정인데..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG-JKQsKxDFo",
        "colab_type": "code",
        "outputId": "fda5d785-b20b-470b-b8bd-08b0e4e84fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.NLLLoss() # multinomial이라서 Cross Entropy 사용 X\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # 랜덤한 텍스트 덩어리를 sampling하고, 이를 index tensor로 변환한다.\n",
        "    input_, label = random_training_set() # chunk_len = 200 이므로, 5의 배수.\n",
        "    hidden = model.init_hidden()\n",
        "    # print(input_.size())\n",
        "    \n",
        "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
        "    optimizer.zero_grad()\n",
        "    for j in range(chunk_len - batch_size): # chunck_len - 1 - (batch_size) + 1\n",
        "        x = torch.stack([input_[j+k:j+k+1] for k in range(batch_size)], dim=0) # [5, 1]\n",
        "        y_ = torch.stack([label[j+k+1:j+k+1+1] for k in range(batch_size)],dim=0) # [5, 1]\n",
        "        y_ = y_.view(-1).long() # [batch_size, 1]\n",
        "        y, hidden = model(x, hidden) # [batch_size, output_size]\n",
        "        # print(y.size(), y_.size())\n",
        "        loss += loss_func(y, y_)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 100 ==0:\n",
        "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
        "        # test를 해야 하는데, batch size가 다를 경우에는 어떻게 inference를 진행하지...?\n",
        "        test_with_batch()\n",
        "        print(\"\\n\",\"=\"*100)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " tensor([2.3296], grad_fn=<DivBackward0>) \n",
            "\n",
            "bbbbbnteo,A hm ties,Nlvd hret ade i a,Ycaeosa yus o hpede\n",
            "o i rsald\n",
            "u hr hsrens i poa o ofcmrnesdisrncu i el hsrnal scl o ofen srltin oceso edet o iei aeetes ad aais etet o nvraetry nton ntefrneuhti o ae a\n",
            "seert frina hpisad lku' adnr o a yulndshried fcmnyadmnc,tin o ilne;bnih fcns o asonlsdsl ec rnprnt o ap'cogtrwndr i ebu?s onA\n",
            "Fritcs nvu\n",
            "ntedos o hvra ut\n",
            "otry\n",
            "nws-ete er hvrue\n",
            "cnvnwyte\n",
            "hvlpatn\n",
            "saes lt\n",
            "es,fcnaes\n",
            "nsogac' pilot,hvriu;I i o ul ortfa.\n",
            "Rmadus ntha hve o otette.\n",
            "Fo o ismndued ecet!gte fll i i ae o aset er at a osey'  o o hvr utaetl o esue fretot o ottn o rwldii:hnie o an hmpeo?\n",
            "eduls\n",
            "nter\n",
            "rntl i'nle i i o ocugnhae' hsathrshpre hdldksae,pnfee so apsmrae;m,Alns o aiptotr,bet o alie ey o ias idnr eee eeeret ntepe,A.\n",
            "Frasec i ikod elccntecmn;bsnpeon efnsfe.\n",
            " cmoh epigs i al o hvrmy.\n",
            "KaGnru\n",
            " ntcaconno ala,A\n",
            "Fr rbedeclutrdeaa\n",
            "hsae\n",
            "yu u i igrg eee i al o acatd\n",
            "sunl i or lsofc uvnssaes pigd\n",
            "s nwrasla\n",
            "ee.\n",
            "Fre sya adscsas o on frt yus\n",
            "al huwe hahl i o ikigt.\n",
            "rtrmd nt\n",
            "hr ay fmceO ee en.GRm amn\n",
            "\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.3342], grad_fn=<DivBackward0>) \n",
            "\n",
            "bbbbbet o a efl a i m,tue hae u a ort aptnrs o i u o apnbe-ate,wrhs rnrpee\n",
            "hyslshfl eke.\n",
            "Gies attrsttbn ee a i ah hth afebn;I o ont hase\n",
            "hyhlit,t,ae o iaepsyhrit,a,w ae eel i o tae-badstd\n",
            "h hvlcntesu,h\n",
            "eto\n",
            " rgae h o agynssafcie,h\n",
            "h  econer ee o acunieecsLSaya:sae o o i hlryih,gd\n",
            "hat rodhrs?\n",
            "hwet h o u a ikfsrt nte h os\n",
            "adod o onrhpr hnwraedd\n",
            "e hvr:apytctswl o i t i,mnlo\n",
            "o o ote\n",
            "hae usIy o isnca\n",
            "sisal ae\n",
            "eto:Tfi u o n esiyd\n",
            "aisrmtrae a agywrnss o on hmie o i o ortt\n",
            "secoo ntaciet\n",
            "o o o uktres wtnhntae!a\n",
            ",te ee adtae.\n",
            "hd o o ohsgo' hr lvite:Ti a amnwrattn sevr  hnwrte yu o u i ee hnwulyac asae.O\n",
            "Scltin \n",
            "yte o o o htmhrigtr\n",
            "o nneoes flcihtt\n",
            "nte hsoor o o oorwre fu-ecss\n",
            "a ulytaeus o oset\n",
            "o rruhe,aeo i i al\n",
            "yuut,hpyhytshnl ftah nte hslfaeitd\n",
            "eyslpno\n",
            " ntl,dy\n",
            "eihe frmnesotia:bste.\n",
            "Poreaieiu,I yu o ueealn\n",
            "\n",
            " iut hnwtrsy\n",
            "h htte' nteaf\n",
            "addnudn\n",
            "h prsrmsrdo,It  nmies o o i pans o.\n",
            " vnwrt,I:Pyuez\n",
            "t i o\n",
            "s yucsie lrwtbns hal ee i ntae ot\n",
            "h hae\n",
            "rsil,betdse\n",
            "snrsen,te hat ed oes hmslnk?\n",
            "Goh rcuge foyts hsae hdd\n",
            "i \n",
            "\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.3933], grad_fn=<DivBackward0>) \n",
            "\n",
            "bbbbbe-e'e\n",
            "o btnsidM-\n",
            "fepoos,Asl hvlud o aeigt,Wrh o o i yuceo i hvlmnie.\n",
            "Btmn hdtrno ee alnauhie:hrntn\n",
            "a orwnlyd o oyf ee hm\n",
            "ol  i xiehto prcmnslnbrt ea hsntie' hsdt ee ote ee admntot hssetgnwrgd nwtde ec\n",
            "edai i i,tnu\n",
            "nttlrt t lvda o auhbrnu hrmgt  o al hm\n",
            "hm upne ntut hah o reea ee h i.S\n",
            "eei adhnwht ussefl\n",
            "o u o edetul ntiet ntmnl o o epe rgipehittrt hwtah\n",
            "adane\n",
            "ntudau saes\n",
            "u o nwlv h i ar nttrtmtrui \n",
            "etrnai u o ee ra,hno o adanet\n",
            "nwrtedhg,fnte evl ae ulnr hvlpnre eee ee yul hvriedh  ueze eieles\n",
            "i ur hrbu a i bdtue,Mcus nwu tieddh\n",
            "ee o o i o orm ntmnwnl?\n",
            " aehto ee,At o or e i o hvrmmnd o i,m ar hrs\n",
            "e teet o i:s nah ote.S\n",
            " oridlnbeo,w,wlon\n",
            "o ien evraie,bne,t h eedtrsela:Wilo.\n",
            "or i taet rfresig'  iunet  rgaeslmns ntek o  aiid htbees bfcmnriet hsns oiedbno,a o tietet i u o ogauh\n",
            "u ee uhnie nte lukt\n",
            "on o,ttrnae rvnyedfrttr rpsndrds o i i ee uat ntaaed otin atreblodddhtdstebu.\n",
            "Dtcnrt o osai,fnue:d tetyadntyah\n",
            " a had  eihl o o rvlntebe?We hvlwrtee\n",
            "o upur atogt i hlcnn:da ntwrt hdad yuaes\n",
            "hssi nsraa\n",
            "\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.2287], grad_fn=<DivBackward0>) \n",
            "\n",
            "bbbbbeg oe ros o i snnse sa erdwrn,I m o ee flt ee o hsalo o osefratin u o o tu osh\n",
            "aa en i rted ruos o hsewnad nttdr ele o i o o o  i ar erin es rih rknwyc' i rkigie?t\n",
            "s o rsrner s eqodethn,wrs  atirly\n",
            "o \n",
            "e o a a er  rfsad\n",
            "segird ful arntr-adig orn o i t eu,is ntigts\n",
            "uebad sae\n",
            "hvrdsae\n",
            "o o ako i erai o  o o o afes o erugsaes n euh  yul ee o acs\n",
            "yure\n",
            "nudshs o hvrvrin  atslsdypdcp tae ee hrwsry\n",
            "h son adtr\n",
            " ntun,I i ae a uos ltnm\n",
            "fi,ntret o evry u a o i:ine ntoh yfrton  tebnad wtwrpe o  a es er o o uvceu,gt nwraeot o se i o i i hrms tel yu,dmrs o urt efreolc euhgshls hmntet sot hwrie a seif\n",
            "o aten o al \n",
            "ettrntshtet i al' t ee.\n",
            "horsfdle\n",
            "'vnahls rcieo\n",
            "o sol i yur ekete rlsigd\n",
            "s plaetrt o nwrbettrs\n",
            "adbettrns'w u o hrs ac hsmnfrd frm e rcvlyas ntemi?\n",
            "h,A uto er.\n",
            "rthu i iyMdd rtzn,atr i o hre,hae\n",
            "flcutae'\n",
            "ery i o huh i ar flsytae\n",
            "adfrms,a;f\n",
            "h o ne odqo,bgdc.N\n",
            "Sylrber\n",
            "o;Bhrd ntaen\n",
            "o o o eselo free\n",
            "syfliehto o ee a o i are\n",
            "ntuea a ee o o ruto o htdrs a or u u irtbr.\n",
            "hreneflrtrs hre\n",
            "ntert,id snwlmnrsTmna.\n",
            "\n",
            " ====================================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-eeaa1e15f50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [5, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, output_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print(y.size(), y_.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-b2309977219b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, hidden)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# input: [batch_size, in_features] = [5 x 100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [2, 5, 100] -> [5, 100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# output: [batch_size, output_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBmI-fKBy_Rk",
        "colab_type": "code",
        "outputId": "854c5dae-b60f-472a-ee94-59be3689fdde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = torch.tensor(22)\n",
        "b = torch.tensor([22])\n",
        "c = torch.tensor([[22]])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([484])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}